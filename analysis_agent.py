import json
import re
import requests
from typing import List, Dict, Any, Optional

class MeetingAnalyzer:
    """
    ä¸€ä¸ªç”¨äºåˆ†æä¼šè®®è®°å½•æ–‡æœ¬çš„ç±»ã€‚

    è¯¥ç±»å¯ä»¥è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼Œå°†é•¿æ–‡æœ¬åˆ†å—ï¼Œè°ƒç”¨AIæ¨¡å‹ï¼ˆé€šä¹‰åƒé—®ï¼‰
    å¯¹æ¯ä¸ªåˆ†å—è¿›è¡Œæ‘˜è¦ï¼Œæœ€åå°†æ‰€æœ‰åˆ†å—çš„æ‘˜è¦æ•´åˆæˆä¸€ä»½éµå¾ª
    ã€Šç½—ä¼¯ç‰¹è®®äº‹è§„åˆ™ã€‹çš„æ­£å¼ä¼šè®®çºªè¦ã€‚

    å±æ€§:
        API_URL (str): é˜¿é‡ŒDashscopeæœåŠ¡çš„APIç«¯ç‚¹ã€‚
        API_MODEL (str): ä½¿ç”¨çš„AIæ¨¡å‹åç§°ã€‚
        DEFAULT_PARAMS (Dict[str, Any]): è°ƒç”¨AIæ—¶çš„é»˜è®¤å‚æ•°ã€‚
        api_key (str): ä»æ–‡ä»¶ä¸­è¯»å–çš„APIå¯†é’¥ã€‚
    """

    API_URL = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
    API_MODEL = "qwen-turbo"
    DEFAULT_PARAMS = {"temperature": 0.3, "max_tokens": 3500}
    CHUNK_MAX_WORDS = 2800

    def __init__(self, api_key_path: str = 'qwen.key'):
        """
        åˆå§‹åŒ– MeetingAnalyzer å®ä¾‹ã€‚

        Args:
            api_key_path (str): å­˜å‚¨APIå¯†é’¥çš„æ–‡ä»¶è·¯å¾„ã€‚

        Raises:
            FileNotFoundError: å¦‚æœAPIå¯†é’¥æ–‡ä»¶ä¸å­˜åœ¨ã€‚
            ValueError: å¦‚æœAPIå¯†é’¥æ–‡ä»¶ä¸ºç©ºã€‚
        """
        try:
            with open(api_key_path, 'r') as f:
                self.api_key = f.read().strip()
            if not self.api_key:
                raise ValueError(f"API key file '{api_key_path}' is empty.")
        except FileNotFoundError:
            print(f"Error: API key file not found at '{api_key_path}'.")
            raise
        except Exception as e:
            print(f"An error occurred while reading the API key: {e}")
            raise

    def _call_ai(self, prompt: str, params: Optional[Dict[str, Any]] = None) -> str:
        """
        å‘AIæ¨¡å‹å‘é€è¯·æ±‚å¹¶è·å–å“åº”ã€‚

        Args:
            prompt (str): å‘é€ç»™AIçš„å®Œæ•´æç¤ºã€‚
            params (Optional[Dict[str, Any]]): æ­¤æ¬¡è°ƒç”¨ç‰¹å®šçš„APIå‚æ•°ï¼Œå¯è¦†ç›–é»˜è®¤å€¼ã€‚

        Returns:
            str: AIæ¨¡å‹çš„å“åº”å†…å®¹ã€‚å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œåˆ™è¿”å›ä¸€ä¸ªåŒ…å«é”™è¯¯ä¿¡æ¯çš„å­—ç¬¦ä¸²ã€‚
        """
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        current_params = self.DEFAULT_PARAMS.copy()
        if params:
            current_params.update(params)
            
        data = {
            "model": self.API_MODEL,
            "input": {"messages": [{"role": "user", "content": prompt}]},
            "parameters": current_params
        }
        
        try:
            response = requests.post(self.API_URL, headers=headers, json=data, timeout=120)
            response.raise_for_status()

            result = response.json()
            output = result.get('output', {})
            
            if 'choices' in output and output['choices']:
                return output['choices'][0]['message']['content']
            elif 'text' in output:
                return output['text']
            
            print(f"âš ï¸ Unexpected API response structure: {result}")
            return "ERROR: Unexpected API response structure"

        except requests.exceptions.RequestException as e:
            print(f"âŒ Request Exception: {e}")
            return f"ERROR: Request Exception - {e}"
        except Exception as e:
            print(f"âŒ AI Call Exception: {e}")
            return f"ERROR: AI Call Exception - {e}"

    @staticmethod
    def _detect_language(text: str) -> str:
        """
        æ ¹æ®æ–‡æœ¬ä¸­ä¸­è‹±æ–‡å­—ç¬¦çš„æ•°é‡ï¼Œç®€å•åˆ¤æ–­æ–‡æœ¬çš„ä¸»è¦è¯­è¨€ã€‚

        Args:
            text (str): å¾…æ£€æµ‹çš„æ–‡æœ¬ã€‚

        Returns:
            str: "ä¸­æ–‡" æˆ– "è‹±æ–‡"ã€‚
        """
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        english_chars = re.findall(r'[a-zA-Z]', text)
        
        if len(chinese_chars) > len(english_chars):
            return "ä¸­æ–‡"
        elif len(english_chars) > len(chinese_chars) and len(english_chars) > 20:
            return "è‹±æ–‡"
        elif len(chinese_chars) > 0:
            return "ä¸­æ–‡"
        else:
            return "è‹±æ–‡"

    @staticmethod
    def _split_text_into_chunks(text: str, max_words: int) -> List[str]:
        """
        å°†æ–‡æœ¬æŒ‰å¤§è‡´çš„å•è¯æ•°é‡åˆ†å‰²æˆå¤šä¸ªå—ã€‚

        Args:
            text (str): åŸå§‹æ–‡æœ¬ã€‚
            max_words (int): æ¯ä¸ªå—çš„æœ€å¤§å•è¯æ•°ã€‚

        Returns:
            List[str]: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨ã€‚
        """
        words = text.split()
        if not words:
            return []
        
        chunks = [" ".join(words[i:i + max_words]) for i in range(0, len(words), max_words)]
        print(f"Text split into {len(chunks)} chunk(s).")
        return chunks

    def _get_summarize_prompt(self, chunk_text: str, is_chinese: bool) -> str:
        """
        æ ¹æ®è¯­è¨€ç”Ÿæˆç”¨äºæ‘˜è¦å•ä¸ªæ–‡æœ¬å—çš„æç¤ºã€‚

        Args:
            chunk_text (str): ä¼šè®®æ–‡æœ¬çš„ä¸€ä¸ªåˆ†å—ã€‚
            is_chinese (bool): å¦‚æœç›®æ ‡è¯­è¨€æ˜¯ä¸­æ–‡ï¼Œåˆ™ä¸ºTrueã€‚

        Returns:
            str: æ ¼å¼åŒ–åçš„å®Œæ•´æç¤ºã€‚
        """
        if is_chinese:
            return f"""
- ä½ æ˜¯ä¸€ä¸ªä¸€ä¸ä¸è‹Ÿä¸”ç«‹åœºä¸­ç«‹çš„ä¼šè®®è®°å½•å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼éµå¾ªã€Šç½—ä¼¯ç‰¹è®®äº‹è§„åˆ™ã€‹çš„åŸåˆ™ï¼Œç”Ÿæˆæ­£å¼çš„ä¼šè®®è®°å½•ã€‚ä¼šè®®è®°å½•å¿…é¡»æ¸…æ™°ã€ç®€æ´ã€å‡†ç¡®åœ°è®°å½•ä¼šè®®ä¸Šâ€œåšäº†ä»€ä¹ˆâ€ï¼Œè€Œä¸æ˜¯â€œè¯´äº†ä»€ä¹ˆâ€ã€‚

- æ“ä½œæŒ‡å—ï¼š
1.  **åŸºæœ¬ä¿¡æ¯**ï¼šåœ¨çºªè¦å¼€å¤´åŒ…å«ç»„ç»‡åç§°ã€ä¼šè®®ç±»å‹ã€æ—¥æœŸã€å¼€å§‹æ—¶é—´å’Œåœ°ç‚¹ã€‚
2.  **å‡ºå¸­æƒ…å†µ**ï¼šåˆ—å‡ºæ‰€æœ‰å‡ºå¸­ã€ç¼ºå¸­çš„æˆå‘˜ï¼Œå¹¶æ³¨æ˜ä¼šè®®è¾¾åˆ°æ³•å®šäººæ•°ã€ä¸»æŒäººåŠè®°å½•å‘˜èº«ä»½ã€‚
3.  **æ‰¹å‡†ä¸Šæ¬¡ä¼šè®®è®°å½•**ï¼šè¯´æ˜ä¸Šæ¬¡ä¼šè®®è®°å½•æ˜¯å¦è¢«å®£è¯»å’Œé€šè¿‡ï¼Œå¹¶è®°å½•ä»»ä½•ä¿®æ­£ã€‚
4.  **æŠ¥å‘Š**ï¼šè®°å½•æ‰€æœ‰æäº¤çš„æŠ¥å‘Šï¼ŒåŒ…æ‹¬æŠ¥å‘Šäººå§“ååŠå¯¹æŠ¥å‘Šé‡‡å–çš„è¡ŒåŠ¨ã€‚
5.  **åŠ¨è®® (Motions)**ï¼šå¯¹äºæ¯ä¸€é¡¹åŠ¨è®®ï¼Œç²¾ç¡®è®°å½•å…¶ç¡®åˆ‡æªè¾ã€åŠ¨è®®äººã€è¡¨å†³ç»“æœï¼ˆé€šè¿‡/æœªé€šè¿‡ï¼‰ä»¥åŠå…·ä½“çš„ç¥¨æ•°ç»Ÿè®¡ï¼ˆèµæˆã€åå¯¹ã€å¼ƒæƒï¼‰ã€‚
6.  **ä¼‘ä¼š (Adjournment)**ï¼šè®°å½•ä¼šè®®çš„ä¼‘ä¼šæ—¶é—´ã€‚
7.  **è¯­è¨€ä¸æ ¼å¼**ï¼šä½¿ç”¨æ­£å¼ã€å®¢è§‚çš„è¿‡å»æ—¶æ€ã€‚æŒ‰ç…§ä¼šè®®è®®ç¨‹çš„é€»è¾‘é¡ºåºç»„ç»‡ä¼šè®®è®°å½•ã€‚

- ä¼šè®®æ–‡å­—è®°å½•å¦‚ä¸‹: 
---
{chunk_text}
---
è¯·æ ¹æ®ä»¥ä¸Šè§„åˆ™ç”Ÿæˆæ­£å¼çš„ä¼šè®®è®°å½•ã€‚
"""
        else:
            return f"""
- You are a meticulous and impartial Meeting Minutes Agent. Your task is to generate formal meeting minutes that strictly adhere to the principles of Robert's Rules of Order. The minutes must be a clear, concise, and accurate record of what was *done* at the meeting, not what was *said*.

- Instructions:
1.  **Essential Information**: Begin with the organization name, meeting type, date, start time, and location.
2.  **Attendance**: List all members present and absent. Note the presence of a quorum and identify the presiding officer and secretary.
3.  **Approval of Previous Minutes**: State if the previous minutes were read and approved, and record any corrections.
4.  **Reports**: Document any reports presented, including the presenter's name and any action taken.
5.  **Motions**: For every motion, record its exact wording, the mover's name, the outcome of the vote (e.g., carried, failed), and the vote count (in favor, against, abstentions).
6.  **Adjournment**: Record the time the meeting was adjourned.
7.  **Language and Format**: Use formal, objective, past-tense language. Organize the minutes logically according to the meeting agenda.

- Original text content to be processed:
---
{chunk_text}
---
Please generate the formal meeting minutes based on the rules above.
"""

    def _summarize_chunk(self, chunk_text: str, is_chinese: bool) -> str:
        """
        å¯¹å•ä¸ªæ–‡æœ¬å—è¿›è¡Œæ‘˜è¦ã€‚

        Args:
            chunk_text (str): å¾…æ‘˜è¦çš„æ–‡æœ¬å—ã€‚
            is_chinese (bool): æ˜¯å¦ä½¿ç”¨ä¸­æ–‡æç¤ºã€‚

        Returns:
            str: æ‘˜è¦ç»“æœæˆ–é”™è¯¯ä¿¡æ¯ã€‚
        """
        prompt = self._get_summarize_prompt(chunk_text, is_chinese)
        summary = self._call_ai(prompt)
        
        if summary.startswith("ERROR:"):
            error_topic = "åˆ†å—æ‘˜è¦å¤±è´¥" if is_chinese else "Chunk Summary Failed"
            error_content = f"æœªèƒ½å¤„ç†æ­¤æ–‡æœ¬å—ã€‚é”™è¯¯ï¼š{summary}" if is_chinese else f"Could not process this text block. Error: {summary}"
            return f"**{error_topic}**\n\n{error_content}"
            
        return summary.strip()

    def _get_consolidation_prompt(self, combined_summaries: str, is_chinese: bool) -> str:
        """
        ç”Ÿæˆç”¨äºæ•´åˆå¤šä¸ªæ‘˜è¦çš„æç¤ºã€‚

        Args:
            combined_summaries (str): ç”±åˆ†éš”ç¬¦è¿æ¥çš„å¤šä¸ªåˆ†å—æ‘˜è¦ã€‚
            is_chinese (bool): æ˜¯å¦ä½¿ç”¨ä¸­æ–‡æç¤ºã€‚

        Returns:
            str: æ ¼å¼åŒ–åçš„æ•´åˆæç¤ºã€‚
        """
        if is_chinese:
            return f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¼–è¾‘ï¼Œæ“…é•¿æ•´åˆä¸æç‚¼ä¿¡æ¯ã€‚
ä»¥ä¸‹æ˜¯åŒä¸€åœºä¼šè®®ä¸åŒéƒ¨åˆ†çš„ä¼šè®®çºªè¦åˆç¨¿ï¼Œç”±åˆ†éš”ç¬¦ `---` éš”å¼€ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1.  ä»”ç»†é˜…è¯»æ‰€æœ‰ç‰‡æ®µã€‚
2.  å°†å®ƒä»¬æ•´åˆæˆä¸€ä»½ **å•ä¸€ã€å®Œæ•´ä¸”è¿è´¯** çš„ä¼šè®®çºªè¦ã€‚
3.  **æ¶ˆé™¤é‡å¤**ï¼šåˆå¹¶é‡å¤çš„æ¡ç›®ï¼ˆå¦‚åŸºæœ¬ä¿¡æ¯ã€å‡ºå¸­åå•ç­‰ï¼‰ï¼Œç¡®ä¿æ¯ä¸ªä¿¡æ¯åªå‡ºç°ä¸€æ¬¡ã€‚
4.  **æŒ‰é€»è¾‘æ’åº**ï¼šç¡®ä¿æœ€ç»ˆçš„çºªè¦éµå¾ªæ ‡å‡†çš„ä¼šè®®æµç¨‹ï¼ˆåŸºæœ¬ä¿¡æ¯ -> å‡ºå¸­æƒ…å†µ -> æ‰¹å‡†ä¸Šæ¬¡çºªè¦ -> æŠ¥å‘Š -> åŠ¨è®® -> ä¼‘ä¼šï¼‰ã€‚
5.  ä¿æŒã€Šç½—ä¼¯ç‰¹è®®äº‹è§„åˆ™ã€‹æ‰€è¦æ±‚çš„æ­£å¼ã€å®¢è§‚çš„é£æ ¼ã€‚

å¾…æ•´åˆçš„çºªè¦ç‰‡æ®µå¦‚ä¸‹ï¼š
---
{combined_summaries}
---
è¯·è¾“å‡ºä¸€ä»½æ•´åˆã€å»é‡å¹¶æŒ‰é€»è¾‘æ’åºåçš„æœ€ç»ˆä¼šè®®çºªè¦ã€‚
"""
        else:
            return f"""
You are an expert editor skilled in synthesizing and refining information.
Below are several draft sections of minutes from the same meeting, separated by `---`.
Your tasks are to:
1.  Carefully read all segments.
2.  Consolidate them into a **single, cohesive, and complete** set of meeting minutes.
3.  **Eliminate Redundancy**: Merge duplicate entries (like header information, attendance lists, etc.) to ensure each piece of information appears only once.
4.  **Ensure Logical Order**: Organize the final minutes to follow a standard meeting flow (Essential Info -> Attendance -> Approval of Minutes -> Reports -> Motions -> Adjournment).
5.  Maintain the formal, objective style required by Robert's Rules of Order.

The draft minute segments to be consolidated are below:
---
{combined_summaries}
---
Please produce the final, consolidated, deduplicated, and logically ordered meeting minutes.
"""

    def _consolidate_summaries(self, chunk_summaries: List[str], is_chinese: bool) -> str:
        """
        å°†å¤šä¸ªåˆ†å—æ‘˜è¦æ•´åˆæˆä¸€ä»½æœ€ç»ˆçš„ä¼šè®®çºªè¦ã€‚

        Args:
            chunk_summaries (List[str]): åŒ…å«å„åˆ†å—æ‘˜è¦çš„åˆ—è¡¨ã€‚
            is_chinese (bool): æ˜¯å¦ä½¿ç”¨ä¸­æ–‡æç¤ºã€‚

        Returns:
            str: æœ€ç»ˆçš„ã€æ•´åˆåçš„ä¼šè®®çºªè¦æˆ–é”™è¯¯ä¿¡æ¯ã€‚
        """
        combined_text = "\n\n---\n\n".join(chunk_summaries)
        prompt = self._get_consolidation_prompt(combined_text, is_chinese)
        
        consolidation_params = {"max_tokens": 8000}
        final_result = self.call_ai(prompt, params=consolidation_params)
        
        if final_result.startswith("ERROR:"):
            error_header = "### æ•´åˆå¤±è´¥" if is_chinese else "### Consolidation Failed"
            error_message = (f"æ— æ³•å®Œæˆæœ€ç»ˆæ‘˜è¦æ•´åˆã€‚é”™è¯¯ï¼š{final_result}"
                             if is_chinese
                             else f"Could not complete final summary consolidation. Error: {final_result}")
            return f"{error_header}\n{error_message}\n\n**åŸå§‹åˆå¹¶æ‘˜è¦:**\n{combined_text}"
        
        return final_result.strip()

    def analyze(self, meeting_text: str) -> str:
        """
        æ‰§è¡Œå®Œæ•´çš„ä¼šè®®æ–‡æœ¬åˆ†ææµç¨‹ã€‚

        æµç¨‹åŒ…æ‹¬ï¼šè¯­è¨€æ£€æµ‹ -> æ–‡æœ¬åˆ†å— -> åˆ†å—æ‘˜è¦ -> æ•´åˆæ‘˜è¦ã€‚

        Args:
            meeting_text (str): å®Œæ•´çš„ä¼šè®®è®°å½•åŸå§‹å†…å®¹ã€‚

        Returns:
            str: Markdown æ ¼å¼çš„æœ€ç»ˆä¼šè®®çºªè¦ã€‚
        """
        if not meeting_text or not meeting_text.strip():
            return "### é”™è¯¯\nè¾“å…¥çš„ä¼šè®®å†…å®¹ä¸ºç©ºã€‚"

        language = self._detect_language(meeting_text)
        is_chinese = (language == "ä¸­æ–‡")
        print(f"ğŸŒ Detected language: {language}")

        chunks = self._split_text_into_chunks(meeting_text, self.CHUNK_MAX_WORDS)
        if not chunks:
            return "### é”™è¯¯\næ— æ³•å°†æ–‡æœ¬åˆ†å‰²æˆå—ã€‚" if is_chinese else "### Error\nCould not split text into chunks."

        chunk_summaries = []
        for i, chunk in enumerate(chunks):
            print(f"Processing chunk {i + 1}/{len(chunks)}...")
            summary = self._summarize_chunk(chunk, is_chinese)
            chunk_summaries.append(summary)
        
        if len(chunk_summaries) == 1:
            return chunk_summaries[0]
            
        print("Consolidating all summaries...")
        final_summary = self._consolidate_summaries(chunk_summaries, is_chinese)
        
        return final_summary


def analyze_meeting_from_file(file_path: str, api_key_path: str = 'qwen.key') -> str:
    """
    ä»æ–‡ä»¶ä¸­è¯»å–ä¼šè®®å†…å®¹å¹¶è¿›è¡Œåˆ†æçš„è¾…åŠ©å‡½æ•°ã€‚

    Args:
        file_path (str): åŒ…å«ä¼šè®®å†…å®¹çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„ã€‚
        api_key_path (str): APIå¯†é’¥æ–‡ä»¶è·¯å¾„ã€‚

    Returns:
        str: åˆ†æç»“æœæˆ–é”™è¯¯ä¿¡æ¯ã€‚
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        return f"### é”™è¯¯\næ‰¾ä¸åˆ°æ–‡ä»¶ï¼š{file_path}"
    except Exception as e:
        return f"### é”™è¯¯\nè¯»å–æ–‡ä»¶æ—¶å‡ºé”™ï¼š{e}"

    if not content or not content.strip():
        return "### é”™è¯¯\næ–‡ä»¶å†…å®¹ä¸ºç©ºã€‚"
        
    analyzer = MeetingAnalyzer(api_key_path=api_key_path)
    return analyzer.analyze(content)


# --- ä¸»æ‰§è¡Œå…¥å£ ---
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Analyze a meeting transcript and generate formal minutes.")
    parser.add_argument("input_file", type=str, help="The path to the text file containing the meeting transcript.")
    parser.add_argument("--key_file", type=str, default="qwen.key", help="The path to the file containing the API key.")
    parser.add_argument("--output_file", type=str, help="The path to save the generated minutes (optional).")
    
    args = parser.parse_args()
    
    print(f"Starting analysis for '{args.input_file}'...")
    analysis_result = analyze_meeting_from_file(args.input_file, args.key_file)
    
    print("\n--- ANALYSIS RESULT ---\n")
    print(analysis_result)
    
    if args.output_file:
        try:
            with open(args.output_file, 'w', encoding='utf-8') as f:
                f.write(analysis_result)
            print(f"\nâœ… Result successfully saved to '{args.output_file}'.")
        except Exception as e:
            print(f"\nâŒ Error saving result to file: {e}")
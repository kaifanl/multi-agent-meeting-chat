{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6518dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ä¼šè®®åˆ†æåŠ©æ‰‹å¯åŠ¨\n",
      "ğŸ‰ æ¬¢è¿ä½¿ç”¨ä¼šè®®åˆ†æåŠ©æ‰‹ï¼\n",
      "==================================================\n",
      "\n",
      "ğŸ“ è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š\n",
      "1. ç›´æ¥ç²˜è´´ä¼šè®®å†…å®¹\n",
      "2. ä¸Šä¼ æ–‡ä»¶ (æ”¯æŒ .txt, .docx)\n",
      "3. é€€å‡º\n",
      "\n",
      "==================================================\n",
      "ğŸ“‹ è¯·ç²˜è´´ä¼šè®®å†…å®¹ (ç²˜è´´å®ŒæˆåæŒ‰å›è½¦):\n",
      "==================================================\n",
      "âœ… å·²è·å– 39785 å­—ç¬¦çš„ä¼šè®®å†…å®¹\n",
      "\n",
      "ğŸ” æ­£åœ¨åˆ†æä¼šè®®å†…å®¹...\n",
      "ğŸŒ æ£€æµ‹è¯­è¨€: è‹±æ–‡\n",
      "ğŸ”„ åˆ†æè‹±æ–‡ä¼šè®®...\n",
      "âš ï¸ ä¼šè®®å†…å®¹è¾ƒé•¿(39785å­—ç¬¦)ï¼Œå°†åˆ†æå‰8000å­—ç¬¦\n",
      "ğŸ”„ æ­£åœ¨è°ƒç”¨AI...\n",
      "ğŸ“¡ APIå“åº”çŠ¶æ€: 200\n",
      "ğŸ” APIè¿”å›ç»“æ„: ['output', 'usage', 'request_id']\n",
      "ğŸ“‹ outputç»“æ„: ['finish_reason', 'text']\n",
      "\n",
      "==================================================\n",
      "ğŸ“‹ English Meeting Analysis\n",
      "==================================================\n",
      "ğŸ“ Meeting Summary: The meeting discussed Hugging Face's collaboration with Intel to showcase efficient AI model building using Intel's CPUs and AI accelerators. The focus was on demonstrating how to optimize workloads with Optimum Intel and Gaudi 3, emphasizing scalability and performance improvements.\n",
      "\n",
      "ğŸ¯ Key Topics:\n",
      "  â€¢ Hugging Cast Overview (high priority)\n",
      "    - Prepare upcoming episodes â†’ Unclear\n",
      "    - Engage with live audience during demos â†’ Hosts\n",
      "  â€¢ Optimization with Optimum Intel (high priority)\n",
      "    - Develop tutorials for Optimum Intel â†’ Documentation Team\n",
      "    - Test integration with popular models â†’ Core Maintainers\n",
      "  â€¢ Intel Gaudi 3 Accelerator (high priority)\n",
      "    - Launch Gaudi 3 demo environment â†’ Reis\n",
      "    - Create user guides for Gaudi 3 â†’ Technical Writers\n",
      "\n",
      "âœ… Decisions:\n",
      "  â€¢ Decide to prioritize CPU optimization alongside GPU workflows.\n",
      "  â€¢ Choose to highlight Optimum Intel as the primary tool for Intel platform optimization.\n",
      "\n",
      "â­ï¸ Next Steps:\n",
      "  â€¢ Launch Gaudi 3 demo environment\n",
      "  â€¢ Publish blog post on Intel Gaudi 3 optimizations\n",
      "  â€¢ Develop tutorial series for Optimum Intel\n",
      "\n",
      "â“ Unresolved Issues:\n",
      "  â€¢ Clarification on specific assignees for tasks\n",
      "  â€¢ Exact deadlines for certain deliverables\n",
      "\n",
      "==================================================\n",
      "==================================================\n",
      "ğŸ”„ ç¿»è¯‘ä¸­...\n",
      "ğŸ”„ æ­£åœ¨è°ƒç”¨AI...\n",
      "ğŸ“¡ APIå“åº”çŠ¶æ€: 200\n",
      "ğŸ” APIè¿”å›ç»“æ„: ['output', 'usage', 'request_id']\n",
      "ğŸ“‹ outputç»“æ„: ['finish_reason', 'text']\n",
      "âœ… ç¿»è¯‘å®Œæˆï¼\n",
      "\n",
      "==================================================\n",
      "ğŸ“‹ ä¸­æ–‡ä¼šè®®çºªè¦\n",
      "==================================================\n",
      "ğŸ“ ä¼šè®®æ€»ç»“ï¼šä¼šè®®è®¨è®ºäº†Hugging Faceä¸Intelçš„åˆä½œï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Intelçš„CPUå’ŒAIåŠ é€Ÿå™¨é«˜æ•ˆæ„å»ºAIæ¨¡å‹ã€‚é‡ç‚¹åœ¨äºå±•ç¤ºå¦‚ä½•é€šè¿‡Optimum Intelå’ŒGaudi 3ä¼˜åŒ–å·¥ä½œè´Ÿè½½ï¼Œå¼ºè°ƒå¯æ‰©å±•æ€§å’Œæ€§èƒ½æå‡ã€‚\n",
      "\n",
      "ğŸ¯ å…³é”®è®®é¢˜ï¼š\n",
      "  â€¢ Hugging Castæ¦‚è¿° (é‡è¦æ€§ï¼šé«˜)\n",
      "    - å‡†å¤‡å³å°†æ’­å‡ºçš„å‰§é›† â†’ ä¸æ˜ç¡®\n",
      "    - åœ¨æ¼”ç¤ºæœŸé—´ä¸ç°åœºè§‚ä¼—äº’åŠ¨ â†’ ä¸»æŒäºº\n",
      "  â€¢ ä½¿ç”¨Optimum Intelè¿›è¡Œä¼˜åŒ– (é‡è¦æ€§ï¼šé«˜)\n",
      "    - å¼€å‘Optimum Intelæ•™ç¨‹ â†’ æ–‡æ¡£å›¢é˜Ÿ\n",
      "    - æµ‹è¯•ä¸æµè¡Œæ¨¡å‹çš„é›†æˆ â†’ æ ¸å¿ƒç»´æŠ¤äººå‘˜\n",
      "  â€¢ Intel Gaudi 3åŠ é€Ÿå™¨ (é‡è¦æ€§ï¼šé«˜)\n",
      "    - å¯åŠ¨Gaudi 3æ¼”ç¤ºç¯å¢ƒ â†’ Reis\n",
      "    - åˆ›å»ºGaudi 3ç”¨æˆ·æŒ‡å— â†’ æŠ€æœ¯ä½œå®¶\n",
      "\n",
      "âœ… å†³ç­–äº‹é¡¹ï¼š\n",
      "  â€¢ å†³å®šä¼˜å…ˆè€ƒè™‘CPUä¼˜åŒ–ä¸GPUå·¥ä½œæµå¹¶è¡Œå‘å±•ã€‚\n",
      "  â€¢ é€‰æ‹©å°†Optimum Intelä½œä¸ºIntelå¹³å°ä¼˜åŒ–çš„ä¸»è¦å·¥å…·ã€‚\n",
      "\n",
      "â­ï¸ ä¸‹ä¸€æ­¥è®¡åˆ’ï¼š\n",
      "  â€¢ å¯åŠ¨Gaudi 3æ¼”ç¤ºç¯å¢ƒ\n",
      "  â€¢ å‘å¸ƒæœ‰å…³Intel Gaudi 3ä¼˜åŒ–çš„åšå®¢æ–‡ç« \n",
      "  â€¢ å¼€å‘Optimum Intelæ•™ç¨‹ç³»åˆ—\n",
      "\n",
      "â“ æœªè§£å†³é—®é¢˜ï¼š\n",
      "  â€¢ ä»»åŠ¡å…·ä½“è´Ÿè´£äººçš„æ¾„æ¸…\n",
      "  â€¢ æŸäº›äº¤ä»˜ç‰©çš„å…·ä½“æˆªæ­¢æ—¥æœŸ\n",
      "\n",
      "ğŸ¯ åˆ†æå®Œæˆï¼\n",
      "âŒ ä¿å­˜å¤±è´¥: [Errno 30] Read-only file system: 'meeting_analysis_20250529_202705.json'\n",
      "\n",
      "ğŸ“ è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š\n",
      "1. ç›´æ¥ç²˜è´´ä¼šè®®å†…å®¹\n",
      "2. ä¸Šä¼ æ–‡ä»¶ (æ”¯æŒ .txt, .docx)\n",
      "3. é€€å‡º\n",
      "\n",
      "==================================================\n",
      "ğŸ“‹ è¯·ç²˜è´´ä¼šè®®å†…å®¹ (ç²˜è´´å®ŒæˆåæŒ‰å›è½¦):\n",
      "==================================================\n",
      "âŒ æ²¡æœ‰è¾“å…¥å†…å®¹\n",
      "\n",
      "ğŸ“ è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š\n",
      "1. ç›´æ¥ç²˜è´´ä¼šè®®å†…å®¹\n",
      "2. ä¸Šä¼ æ–‡ä»¶ (æ”¯æŒ .txt, .docx)\n",
      "3. é€€å‡º\n",
      "ğŸ‘‹ å†è§ï¼\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class MeetingHelper:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\"\n",
    "    \n",
    "    def start(self):\n",
    "        print(\"ğŸ‰ æ¬¢è¿ä½¿ç”¨ä¼šè®®åˆ†æåŠ©æ‰‹ï¼\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\nğŸ“ è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š\")\n",
    "            print(\"1. ç›´æ¥ç²˜è´´ä¼šè®®å†…å®¹\")\n",
    "            print(\"2. ä¸Šä¼ æ–‡ä»¶ (æ”¯æŒ .txt, .docx)\")\n",
    "            print(\"3. é€€å‡º\")\n",
    "            \n",
    "            choice = input(\"\\nè¯·é€‰æ‹© (1-3): \").strip()\n",
    "            \n",
    "            if choice == \"1\":\n",
    "                meeting_text = self.get_text_input()\n",
    "                if meeting_text:\n",
    "                    self.process_meeting(meeting_text)\n",
    "            elif choice == \"2\":\n",
    "                meeting_text = self.get_file_input()\n",
    "                if meeting_text:\n",
    "                    self.process_meeting(meeting_text)\n",
    "            elif choice == \"3\":\n",
    "                print(\"ğŸ‘‹ å†è§ï¼\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"âŒ è¯·é€‰æ‹© 1-3\")\n",
    "    \n",
    "\n",
    "    \n",
    "    def process_meeting(self, meeting_text):\n",
    "        print(\"\\nğŸ” æ­£åœ¨åˆ†æä¼šè®®å†…å®¹...\")\n",
    "        result = self.analyze(meeting_text)\n",
    "        \n",
    "        if result[\"ok\"]:\n",
    "            print(\"\\nğŸ¯ åˆ†æå®Œæˆï¼\")\n",
    "            save_choice = input(\"\\nğŸ’¾ æ˜¯å¦ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶ï¼Ÿ(y/n): \").lower().strip()\n",
    "            if save_choice in ['y', 'yes', 'æ˜¯', 'å¥½']:\n",
    "                self.save_result(result[\"result\"])\n",
    "        else:\n",
    "            print(f\"âŒ åˆ†æå¤±è´¥: {result['error']}\")\n",
    "    \n",
    "    def save_result(self, result):\n",
    "        try:\n",
    "            filename = f\"meeting_analysis_{self.get_timestamp()}.json\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"âœ… ç»“æœå·²ä¿å­˜åˆ°: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ä¿å­˜å¤±è´¥: {str(e)}\")\n",
    "    \n",
    "    def get_timestamp(self):\n",
    "        from datetime import datetime\n",
    "        return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    def detect_language(self, text):\n",
    "        chinese_chars = len(re.findall(r'[\\u4e00-\\u9fff]', text))\n",
    "        english_chars = len(re.findall(r'[a-zA-Z]', text))\n",
    "        \n",
    "        if chinese_chars > english_chars:\n",
    "            return \"ä¸­æ–‡\"\n",
    "        elif english_chars > chinese_chars * 2:\n",
    "            return \"è‹±æ–‡\"\n",
    "        else:\n",
    "            return \"æ··åˆ\"\n",
    "    \n",
    "    def analyze(self, meeting_text, auto_translate=False):\n",
    "        language = self.detect_language(meeting_text)\n",
    "        print(f\"ğŸŒ æ£€æµ‹è¯­è¨€: {language}\")\n",
    "        \n",
    "        if language == \"è‹±æ–‡\":\n",
    "            print(\"ğŸ”„ åˆ†æè‹±æ–‡ä¼šè®®...\")\n",
    "            english_result = self.analyze_in_english(meeting_text)\n",
    "            \n",
    "            if english_result[\"ok\"]:\n",
    "                if not auto_translate:\n",
    "                    print(\"\\n\" + \"=\"*50)\n",
    "                    print(\"ğŸ“‹ English Meeting Analysis\")\n",
    "                    print(\"=\"*50)\n",
    "                    self.print_result_english(english_result[\"result\"])\n",
    "                    \n",
    "                    print(\"\\n\" + \"=\"*50)\n",
    "                    translate_choice = input(\"ğŸ¤” éœ€è¦ç¿»è¯‘æˆä¸­æ–‡å—ï¼Ÿ(y/n): \").lower().strip()\n",
    "                    print(\"=\"*50)\n",
    "                    \n",
    "                    if translate_choice in ['y', 'yes', 'æ˜¯', 'å¥½']:\n",
    "                        print(\"ğŸ”„ ç¿»è¯‘ä¸­...\")\n",
    "                        chinese_result = self.translate_result_to_chinese(english_result[\"result\"])\n",
    "                        if chinese_result[\"ok\"]:\n",
    "                            print(\"âœ… ç¿»è¯‘å®Œæˆï¼\")\n",
    "                            print(\"\\n\" + \"=\"*50)\n",
    "                            print(\"ğŸ“‹ ä¸­æ–‡ä¼šè®®çºªè¦\")\n",
    "                            print(\"=\"*50)\n",
    "                            self.print_result_chinese(chinese_result[\"result\"])\n",
    "                            return chinese_result\n",
    "                        else:\n",
    "                            print(\"âŒ ç¿»è¯‘å¤±è´¥ï¼Œä¿ç•™è‹±æ–‡ç‰ˆæœ¬\")\n",
    "                            return english_result\n",
    "                    else:\n",
    "                        return english_result\n",
    "                else:\n",
    "                    chinese_result = self.translate_result_to_chinese(english_result[\"result\"])\n",
    "                    return chinese_result if chinese_result[\"ok\"] else english_result\n",
    "            else:\n",
    "                return english_result\n",
    "        else:\n",
    "            print(\"ğŸ”„ åˆ†æä¸­æ–‡ä¼šè®®...\")\n",
    "            chinese_result = self.analyze_in_chinese(meeting_text)\n",
    "            \n",
    "            if chinese_result[\"ok\"]:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"ğŸ“‹ ä¸­æ–‡ä¼šè®®çºªè¦\")\n",
    "                print(\"=\"*50)\n",
    "                self.print_result_chinese(chinese_result[\"result\"])\n",
    "            \n",
    "            return chinese_result\n",
    "    \n",
    "    def analyze_in_english(self, meeting_text):\n",
    "        if len(meeting_text) > 8000:\n",
    "            print(f\"âš ï¸ ä¼šè®®å†…å®¹è¾ƒé•¿({len(meeting_text)}å­—ç¬¦)ï¼Œå°†åˆ†æå‰8000å­—ç¬¦\")\n",
    "            meeting_text = meeting_text[:8000]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a professional meeting minutes assistant. Please extract key information from the following meeting content and return it in JSON format.\n",
    "\n",
    "Meeting Content:\n",
    "{meeting_text}\n",
    "\n",
    "Please generate JSON in the following format:\n",
    "{{\n",
    "  \"meeting_summary\": \"Brief summary of meeting content, 2-3 sentences\",\n",
    "  \"key_topics\": [\n",
    "    {{\n",
    "      \"topic\": \"Topic name\",\n",
    "      \"description\": \"Detailed description of the topic\",\n",
    "      \"importance\": \"high/medium/low\",\n",
    "      \"related_tasks\": [\n",
    "        {{\n",
    "          \"task\": \"Task name under this topic\",\n",
    "          \"assignee\": \"Responsible person\",\n",
    "          \"deadline\": \"Deadline\",\n",
    "          \"priority\": \"high/medium/low\",\n",
    "          \"status\": \"pending/in-progress/completed\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ],\n",
    "  \"decisions\": [\n",
    "    {{\n",
    "      \"decision\": \"Decision content\",\n",
    "      \"decision_maker\": \"Decision maker\",\n",
    "      \"impact\": \"Impact\"\n",
    "    }}\n",
    "  ],\n",
    "  \"next_steps\": [\"Next steps\"],\n",
    "  \"unresolved_issues\": [\"Unresolved issues\"]\n",
    "}}\n",
    "Requirements:\n",
    "- Each topic can contain multiple subtasks.\n",
    "- If information is unclear, fill in \"unclear\".\n",
    "- Return must be strict JSON format without explanations.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.call_ai(prompt)\n",
    "            if response:\n",
    "                data = self.parse_json(response)\n",
    "                if data:\n",
    "                    return {\"ok\": True, \"result\": self.fix_data_english(data)}\n",
    "                else:\n",
    "                    return self.simple_way_english(meeting_text)\n",
    "            else:\n",
    "                return {\"ok\": False, \"error\": \"AIè°ƒç”¨å¤±è´¥\"}\n",
    "        except Exception as e:\n",
    "            return {\"ok\": False, \"error\": f\"åˆ†æå‡ºé”™: {str(e)}\"}\n",
    "    \n",
    "    def analyze_in_chinese(self, meeting_text):\n",
    "        # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œå…ˆæˆªå–å‰é¢éƒ¨åˆ†\n",
    "        if len(meeting_text) > 8000:\n",
    "            print(f\"âš ï¸ ä¼šè®®å†…å®¹è¾ƒé•¿({len(meeting_text)}å­—ç¬¦)ï¼Œå°†åˆ†æå‰8000å­—ç¬¦\")\n",
    "            meeting_text = meeting_text[:8000]\n",
    "            \n",
    "        prompt = f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¼šè®®çºªè¦åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¼šè®®å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ã€‚\n",
    "ä¼šè®®å†…å®¹ï¼š\n",
    "{meeting_text}\n",
    "è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç”ŸæˆJSONï¼š\n",
    "{{\n",
    "  \"meeting_summary\": \"ç®€è¦æ€»ç»“ä¼šè®®å†…å®¹ï¼Œ2-3å¥è¯\",\n",
    "  \"key_topics\": [\n",
    "    {{\n",
    "      \"topic\": \"è®®é¢˜åç§°\",\n",
    "      \"description\": \"è¯¥è®®é¢˜çš„è¯¦ç»†æè¿°\",\n",
    "      \"importance\": \"high/medium/low\",\n",
    "      \"related_tasks\": [\n",
    "        {{\n",
    "          \"task\": \"è¯¥è®®é¢˜ä¸‹çš„å­ä»»åŠ¡åç§°\",\n",
    "          \"assignee\": \"è´Ÿè´£äºº\",\n",
    "          \"deadline\": \"æˆªæ­¢æ—¶é—´\",\n",
    "          \"priority\": \"high/medium/low\",\n",
    "          \"status\": \"å¾…å¤„ç†/è¿›è¡Œä¸­/å·²å®Œæˆ\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ],\n",
    "  \"decisions\": [\n",
    "    {{\n",
    "      \"decision\": \"å†³ç­–å†…å®¹\",\n",
    "      \"decision_maker\": \"å†³ç­–äºº\",\n",
    "      \"impact\": \"å½±å“\"\n",
    "    }}\n",
    "  ],\n",
    "  \"next_steps\": [\"ä¸‹ä¸€æ­¥è®¡åˆ’\"],\n",
    "  \"unresolved_issues\": [\"æœªè§£å†³é—®é¢˜\"]\n",
    "}}\n",
    "è¦æ±‚ï¼š\n",
    "- æ¯ä¸ªè®®é¢˜éƒ½å¯ä»¥åŒ…å«å¤šä¸ªå­ä»»åŠ¡ã€‚\n",
    "- å­ä»»åŠ¡å­—æ®µä¸­ï¼Œå¦‚æœä¿¡æ¯ä¸æ˜ç¡®ï¼Œè¯·å¡«å†™\"æœªæ˜ç¡®\"ã€‚\n",
    "- è¿”å›ç»“æœå¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«è§£é‡Šè¯´æ˜æˆ–å¤šä½™æ–‡å­—ã€‚\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.call_ai(prompt)\n",
    "            if response:\n",
    "                data = self.parse_json(response)\n",
    "                if data:\n",
    "                    return {\"ok\": True, \"result\": self.fix_data_chinese(data)}\n",
    "                else:\n",
    "                    return self.simple_way_chinese(meeting_text)\n",
    "            else:\n",
    "                return {\"ok\": False, \"error\": \"AIè°ƒç”¨å¤±è´¥\"}\n",
    "        except Exception as e:\n",
    "            return {\"ok\": False, \"error\": f\"åˆ†æå‡ºé”™: {str(e)}\"}\n",
    "    \n",
    "    def call_ai(self, prompt):\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {self.api_key}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"qwen-turbo\",\n",
    "            \"input\": {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
    "            \"parameters\": {\"temperature\": 0.3, \"max_tokens\": 2000}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(\"ğŸ”„ æ­£åœ¨è°ƒç”¨AI...\")\n",
    "            resp = requests.post(self.api_url, headers=headers, json=data)\n",
    "            print(f\"ğŸ“¡ APIå“åº”çŠ¶æ€: {resp.status_code}\")\n",
    "            \n",
    "            if resp.status_code == 200:\n",
    "                result = resp.json()\n",
    "                print(f\"ğŸ” APIè¿”å›ç»“æ„: {list(result.keys())}\")\n",
    "                \n",
    "                # è°ƒè¯•ï¼šæ‰“å°å®Œæ•´å“åº”ç»“æ„\n",
    "                if 'output' in result:\n",
    "                    print(f\"ğŸ“‹ outputç»“æ„: {list(result['output'].keys())}\")\n",
    "                    if 'choices' in result['output']:\n",
    "                        return result['output']['choices'][0]['message']['content']\n",
    "                    elif 'text' in result['output']:\n",
    "                        return result['output']['text']\n",
    "                    else:\n",
    "                        print(f\"âš ï¸ æœªæ‰¾åˆ°é¢„æœŸå­—æ®µï¼Œå®Œæ•´å“åº”: {result}\")\n",
    "                        return None\n",
    "                else:\n",
    "                    print(f\"âš ï¸ æ²¡æœ‰outputå­—æ®µï¼Œå®Œæ•´å“åº”: {result}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"âŒ APIé”™è¯¯: {resp.status_code}\")\n",
    "                print(f\"é”™è¯¯å†…å®¹: {resp.text}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_json(self, text):\n",
    "        try:\n",
    "            clean_text = text.strip()\n",
    "            if clean_text.startswith('```json'):\n",
    "                clean_text = clean_text.split('```json')[1]\n",
    "            if clean_text.endswith('```'):\n",
    "                clean_text = clean_text.rsplit('```', 1)[0]\n",
    "            return json.loads(clean_text)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def translate_result_to_chinese(self, english_result):\n",
    "        translate_prompt = f\"\"\"\n",
    "è¯·å°†ä»¥ä¸‹è‹±æ–‡ä¼šè®®åˆ†æç»“æœç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒJSONç»“æ„ä¸å˜ï¼š\n",
    "\n",
    "{json.dumps(english_result, indent=2)}\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "- ä¿æŒJSONæ ¼å¼å®Œå…¨ä¸€è‡´\n",
    "- åªç¿»è¯‘æ–‡æœ¬å†…å®¹ï¼Œä¸æ”¹å˜å­—æ®µå\n",
    "- ç¿»è¯‘è¦è‡ªç„¶æµç•…\n",
    "- ç›´æ¥è¿”å›ç¿»è¯‘åçš„JSONï¼Œä¸è¦è§£é‡Š\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.call_ai(translate_prompt)\n",
    "            if response:\n",
    "                translated_data = self.parse_json(response)\n",
    "                if translated_data:\n",
    "                    return {\"ok\": True, \"result\": translated_data}\n",
    "            return {\"ok\": False, \"error\": \"ç¿»è¯‘å¤±è´¥\"}\n",
    "        except:\n",
    "            return {\"ok\": False, \"error\": \"ç¿»è¯‘å‡ºé”™\"}\n",
    "    \n",
    "    def fix_data_english(self, data):\n",
    "        basic_fields = [\"meeting_summary\", \"key_topics\", \"decisions\", \"next_steps\", \"unresolved_issues\"]\n",
    "        for field in basic_fields:\n",
    "            if field not in data:\n",
    "                data[field] = [] if field != \"meeting_summary\" else \"\"\n",
    "        \n",
    "        for i, topic in enumerate(data.get(\"key_topics\", [])):\n",
    "            topic[\"id\"] = f\"topic_{i+1}\"\n",
    "            if \"related_tasks\" not in topic:\n",
    "                topic[\"related_tasks\"] = []\n",
    "            for j, task in enumerate(topic[\"related_tasks\"]):\n",
    "                task[\"id\"] = f\"task_{i+1}_{j+1}\"\n",
    "                task.setdefault(\"status\", \"pending\")\n",
    "                task.setdefault(\"assignee\", \"unassigned\")\n",
    "                task.setdefault(\"deadline\", \"unclear\")\n",
    "                task.setdefault(\"priority\", \"medium\")\n",
    "        \n",
    "        for decision in data.get(\"decisions\", []):\n",
    "            decision.setdefault(\"decision_maker\", \"unclear\")\n",
    "            decision.setdefault(\"impact\", \"unclear\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def fix_data_chinese(self, data):\n",
    "        basic_fields = [\"meeting_summary\", \"key_topics\", \"decisions\", \"next_steps\", \"unresolved_issues\"]\n",
    "        for field in basic_fields:\n",
    "            if field not in data:\n",
    "                data[field] = [] if field != \"meeting_summary\" else \"\"\n",
    "        \n",
    "        for i, topic in enumerate(data.get(\"key_topics\", [])):\n",
    "            topic[\"id\"] = f\"è®®é¢˜_{i+1}\"\n",
    "            if \"related_tasks\" not in topic:\n",
    "                topic[\"related_tasks\"] = []\n",
    "            for j, task in enumerate(topic[\"related_tasks\"]):\n",
    "                task[\"id\"] = f\"ä»»åŠ¡_{i+1}_{j+1}\"\n",
    "                task.setdefault(\"status\", \"å¾…å¤„ç†\")\n",
    "                task.setdefault(\"assignee\", \"æœªæ˜ç¡®\")\n",
    "                task.setdefault(\"deadline\", \"æœªæ˜ç¡®\")\n",
    "                task.setdefault(\"priority\", \"medium\")\n",
    "        \n",
    "        for decision in data.get(\"decisions\", []):\n",
    "            decision.setdefault(\"decision_maker\", \"æœªæ˜ç¡®\")\n",
    "            decision.setdefault(\"impact\", \"æœªæ˜ç¡®\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def simple_way_english(self, meeting_text):\n",
    "        try:\n",
    "            topic_answer = self.call_ai(f\"What are the main topics?\\n{meeting_text[:800]}\")\n",
    "            decision_answer = self.call_ai(f\"What decisions were made?\\n{meeting_text[:800]}\")\n",
    "            \n",
    "            return {\n",
    "                \"ok\": True,\n",
    "                \"result\": {\n",
    "                    \"meeting_summary\": \"Meeting analysis completed\",\n",
    "                    \"key_topics\": [{\n",
    "                        \"id\": \"topic_1\",\n",
    "                        \"topic\": \"Main Topics\", \n",
    "                        \"description\": topic_answer or \"Unable to extract\",\n",
    "                        \"importance\": \"medium\",\n",
    "                        \"related_tasks\": []\n",
    "                    }],\n",
    "                    \"decisions\": [{\n",
    "                        \"decision\": decision_answer or \"No clear decisions\", \n",
    "                        \"decision_maker\": \"unclear\", \n",
    "                        \"impact\": \"unclear\"\n",
    "                    }],\n",
    "                    \"next_steps\": [],\n",
    "                    \"unresolved_issues\": []\n",
    "                }\n",
    "            }\n",
    "        except:\n",
    "            return {\"ok\": False, \"error\": \"Simple extraction failed\"}\n",
    "    \n",
    "    def simple_way_chinese(self, meeting_text):\n",
    "        try:\n",
    "            topic_answer = self.call_ai(f\"ä¸»è¦è®¨è®ºäº†ä»€ä¹ˆï¼Ÿ\\n{meeting_text[:800]}\")\n",
    "            decision_answer = self.call_ai(f\"åšäº†ä»€ä¹ˆå†³å®šï¼Ÿ\\n{meeting_text[:800]}\")\n",
    "            \n",
    "            return {\n",
    "                \"ok\": True,\n",
    "                \"result\": {\n",
    "                    \"meeting_summary\": \"ä¼šè®®åˆ†æå®Œæˆ\",\n",
    "                    \"key_topics\": [{\n",
    "                        \"id\": \"è®®é¢˜_1\",\n",
    "                        \"topic\": \"ä¸»è¦è®®é¢˜\", \n",
    "                        \"description\": topic_answer or \"æ— æ³•æå–\",\n",
    "                        \"importance\": \"medium\",\n",
    "                        \"related_tasks\": []\n",
    "                    }],\n",
    "                    \"decisions\": [{\n",
    "                        \"decision\": decision_answer or \"æ— æ˜ç¡®å†³ç­–\", \n",
    "                        \"decision_maker\": \"æœªæ˜ç¡®\", \n",
    "                        \"impact\": \"æœªæ˜ç¡®\"\n",
    "                    }],\n",
    "                    \"next_steps\": [],\n",
    "                    \"unresolved_issues\": []\n",
    "                }\n",
    "            }\n",
    "        except:\n",
    "            return {\"ok\": False, \"error\": \"ç®€å•æå–å¤±è´¥\"}\n",
    "    \n",
    "    def print_result_english(self, result):\n",
    "        print(f\"ğŸ“ Meeting Summary: {result.get('meeting_summary', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\nğŸ¯ Key Topics:\")\n",
    "        for topic in result.get('key_topics', []):\n",
    "            print(f\"  â€¢ {topic.get('topic', 'Unknown')} ({topic.get('importance', 'medium')} priority)\")\n",
    "            if topic.get('related_tasks'):\n",
    "                for task in topic['related_tasks']:\n",
    "                    print(f\"    - {task.get('task', 'Unknown task')} â†’ {task.get('assignee', 'Unassigned')}\")\n",
    "        \n",
    "        print(\"\\nâœ… Decisions:\")\n",
    "        for decision in result.get('decisions', []):\n",
    "            print(f\"  â€¢ {decision.get('decision', 'Unknown decision')}\")\n",
    "        \n",
    "        if result.get('next_steps'):\n",
    "            print(\"\\nâ­ï¸ Next Steps:\")\n",
    "            for step in result['next_steps']:\n",
    "                print(f\"  â€¢ {step}\")\n",
    "        \n",
    "        if result.get('unresolved_issues'):\n",
    "            print(\"\\nâ“ Unresolved Issues:\")\n",
    "            for issue in result['unresolved_issues']:\n",
    "                print(f\"  â€¢ {issue}\")\n",
    "    \n",
    "    def print_result_chinese(self, result):\n",
    "        print(f\"ğŸ“ ä¼šè®®æ€»ç»“ï¼š{result.get('meeting_summary', 'æ— ')}\")\n",
    "        \n",
    "        print(\"\\nğŸ¯ å…³é”®è®®é¢˜ï¼š\")\n",
    "        for topic in result.get('key_topics', []):\n",
    "            print(f\"  â€¢ {topic.get('topic', 'æœªçŸ¥è®®é¢˜')} (é‡è¦æ€§ï¼š{topic.get('importance', 'medium')})\")\n",
    "            if topic.get('related_tasks'):\n",
    "                for task in topic['related_tasks']:\n",
    "                    print(f\"    - {task.get('task', 'æœªçŸ¥ä»»åŠ¡')} â†’ {task.get('assignee', 'æœªåˆ†é…')}\")\n",
    "        \n",
    "        print(\"\\nâœ… å†³ç­–äº‹é¡¹ï¼š\")\n",
    "        for decision in result.get('decisions', []):\n",
    "            print(f\"  â€¢ {decision.get('decision', 'æœªçŸ¥å†³ç­–')}\")\n",
    "        \n",
    "        if result.get('next_steps'):\n",
    "            print(\"\\nâ­ï¸ ä¸‹ä¸€æ­¥è®¡åˆ’ï¼š\")\n",
    "            for step in result['next_steps']:\n",
    "                print(f\"  â€¢ {step}\")\n",
    "        \n",
    "        if result.get('unresolved_issues'):\n",
    "            print(\"\\nâ“ æœªè§£å†³é—®é¢˜ï¼š\")\n",
    "            for issue in result['unresolved_issues']:\n",
    "                print(f\"  â€¢ {issue}\")\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸš€ ä¼šè®®åˆ†æåŠ©æ‰‹å¯åŠ¨\")\n",
    "    \n",
    "    api_key = input(\"è¯·è¾“å…¥åƒé—®APIå¯†é’¥: \").strip()\n",
    "    if not api_key:\n",
    "        print(\"âŒ APIå¯†é’¥ä¸èƒ½ä¸ºç©º\")\n",
    "        return\n",
    "    \n",
    "    helper = MeetingHelper(api_key)\n",
    "    helper.start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffd65f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Meeting Analysis Tool (V2 - Markdown Output)\n",
      "==================================================\n",
      "\n",
      "--- Analyzing English Meeting Text (expecting multiple chunks) ---\n",
      "\n",
      "Analysis Result (Markdown String - English):\n",
      "### Topic: Efficient AI Model Deployment Using Intel Platforms  \n",
      "\n",
      "The episode explores the collaboration between Hugging Face and Intel to optimize AI model deployment on Intel CPUs and AI accelerators. Intel's fourth-generation Xeon Scalable processors with Advanced Matrix Extensions (AMX) accelerate matrix multiplications and support data types like BFloat16 and Integer 8. The third-generation Gaudi AI accelerator (Gaudi 3) features 128 GB of RAM and enhanced performance in FP8 and BFloat16 formats. Optimum Intel and Optimum Habana libraries enable seamless integration of these technologies into existing workflows. A live demo showcased Text Generation Inference (TGI) on Gaudi 3 for deploying large language models like Llama 3.17B efficiently. The partnership focuses on providing scalable and efficient solutions for enterprise applications, such as retrieval-augmented generation (RAG), with CPUs offering versatility for diverse machine learning tasks.\n",
      "\n",
      "---\n",
      "\n",
      "### Topic: Accelerating GPT-LLM Deployments on Intel Xeon CPUs Using Optimum Intel  \n",
      "\n",
      "The discussion centers on accelerating generative large language model (GPT-LLM) deployments on Intel Xeon CPUs using Optimum Intel. Key aspects include exporting models to OpenVINO Intermediate Representation (IR), applying weight-only quantization (8-bit and 4-bit), and performing inference with latent consistency models. Creating an OpenVINO-compatible model involves setting public or private repositories, reducing model size from FP32 to INT8 (four times smaller). Quantization methods like 8-bit and 4-bit were explored, with options for dataset calibration and controlling the ratio of weights quantized to 4-bit vs. 8-bit to mitigate accuracy loss. Optimum Benchmark, a tool for benchmarking performance across frameworks (PyTorch, OpenVINO, etc.), was introduced. The benchmarking process includes static shape compilation, weight-only quantization, and numactl support for multi-node CPUs. Results demonstrate significant speedups, especially in the decode stage of models like GPT-2, achieving up to 2.5x improvements in throughput and 7x speedup in per-token latency. The session underscores the importance of benchmarking to identify the optimal framework for specific use cases.\n",
      "\n",
      "---\n",
      "\n",
      "### Topic: Overview of Intel AI Accelerators and Model Optimization Techniques  \n",
      "\n",
      "The discussion delves into Intel AI accelerators (Gaudi) as alternatives to GPUs for AI workloads, focusing on the latest G3 generation. These accelerators are supported in libraries like Optimum Habana and production inference solutions such as TGI. Features include FP8 support, multimodal models, and deterministic or sampled outputs for inference requests. Optimum Intel accelerates AI workloads on CPUs and GPUs using frameworks like Intel Extensions for PyTorch (IPEx) and OpenVINO. Topics covered include exporting models for graph-mode execution, integrating with Hugging Face ecosystems, and optimizing inference through quantization. Quantization methods discussed include weight-only, dynamic, and static approaches, with considerations for memory efficiency, speed, and energy consumption. The impact of quantization on model accuracy varies based on techniques and model characteristics, with examples showing potential improvements in certain cases.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "# os and pathlib are not needed\n",
    "# from datetime import datetime # Not needed\n",
    "\n",
    "class MeetingAnalyzerV2:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\"\n",
    "        # Increased max_tokens as summaries can be long, especially combined ones\n",
    "        self.default_parameters = {\"temperature\": 0.3, \"max_tokens\": 3500} \n",
    "\n",
    "    def detect_language(self, text):\n",
    "        # Simple language detection based on character counts\n",
    "        chinese_chars = len(re.findall(r'[\\u4e00-\\u9fff]', text))\n",
    "        english_chars = len(re.findall(r'[a-zA-Z]', text))\n",
    "        \n",
    "        if chinese_chars > english_chars:\n",
    "            return \"ä¸­æ–‡\"\n",
    "        elif english_chars > chinese_chars and english_chars > 20:\n",
    "            return \"è‹±æ–‡\"\n",
    "        elif chinese_chars > 0 :\n",
    "            return \"ä¸­æ–‡\"\n",
    "        else:\n",
    "            return \"è‹±æ–‡\"\n",
    "\n",
    "    def call_ai(self, prompt, parameters=None):\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {self.api_key}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        \n",
    "        current_parameters = self.default_parameters.copy()\n",
    "        if parameters:\n",
    "            current_parameters.update(parameters)\n",
    "            \n",
    "        data = {\n",
    "            \"model\": \"qwen-turbo\", # or qwen-long for very long contexts if needed/available\n",
    "            \"input\": {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
    "            \"parameters\": current_parameters\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # print(f\"ğŸ”„ Calling AI with prompt (first 100 chars): {prompt[:100]}...\")\n",
    "            resp = requests.post(self.api_url, headers=headers, json=data, timeout=120) # Increased timeout\n",
    "            # print(f\"ğŸ“¡ API Status: {resp.status_code}\")\n",
    "            \n",
    "            if resp.status_code == 200:\n",
    "                result = resp.json()\n",
    "                if 'output' in result:\n",
    "                    if 'choices' in result['output'] and result['output']['choices']:\n",
    "                        return result['output']['choices'][0]['message']['content']\n",
    "                    elif 'text' in result['output']:\n",
    "                        return result['output']['text']\n",
    "                print(f\"âš ï¸ Unexpected API response structure: {result}\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"âŒ API Error: {resp.status_code} - {resp.text}\")\n",
    "                return f\"API_ERROR: {resp.status_code} - {resp.text}\" # Return error string\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ Request Exception: {str(e)}\")\n",
    "            return f\"REQUEST_EXCEPTION: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ AI Call Exception: {str(e)}\")\n",
    "            return f\"AI_CALL_EXCEPTION: {str(e)}\"\n",
    "        return None\n",
    "\n",
    "\n",
    "    def _split_text_into_chunks(self, text: str, max_words: int = 3000) -> list[str]:\n",
    "        \"\"\"Splits text into chunks of approximately max_words.\"\"\"\n",
    "        words = text.split() # Simple split by whitespace\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), max_words):\n",
    "            chunk_words = words[i:i + max_words]\n",
    "            chunks.append(\" \".join(chunk_words))\n",
    "        # print(f\"Split text into {len(chunks)} chunks.\")\n",
    "        return chunks\n",
    "\n",
    "    def _summarize_chunk_in_language(self, chunk_text: str, is_chinese_prompt: bool) -> str:\n",
    "        \"\"\"Summarizes a single chunk of text using Markdown format.\"\"\"\n",
    "        if is_chinese_prompt:\n",
    "            prompt = f\"\"\"\n",
    "è¯·ä½ æ‰®æ¼”ä¸€ä¸ªä¸“ä¸šçš„ä¼šè®®è®°å½•å‘˜ã€‚\n",
    "é’ˆå¯¹ä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µï¼Œè¯·æå–æ ¸å¿ƒè®®é¢˜å’Œå…³é”®å†…å®¹ã€‚\n",
    "ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šã€å¼•è¨€æˆ–ç»“å°¾ï¼š\n",
    "\n",
    "**ä¸»é¢˜ï¼š** [æ­¤å¤„å¡«å†™è¯†åˆ«å‡ºçš„ä¸»é¢˜]\n",
    "**å…·ä½“å†…å®¹ï¼š** [æ­¤å¤„å¡«å†™è¯¥ä¸»é¢˜ä¸‹çš„è¯¦ç»†å†…å®¹æ‘˜è¦]\n",
    "\n",
    "æ–‡æœ¬ç‰‡æ®µï¼š\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "è¯·ç¡®ä¿è¾“å‡ºæ¸…æ™°ã€ç®€æ´ï¼Œå¹¶ä¸¥æ ¼éµå®ˆä¸Šè¿°æ ¼å¼ã€‚\n",
    "\"\"\"\n",
    "        else: # English prompt\n",
    "            prompt = f\"\"\"\n",
    "You are a professional meeting summarizer.\n",
    "For the following text segment, extract the core topic and key content.\n",
    "Strictly use the following Markdown format for your output. Do not add any extra explanations, introductions, or conclusions:\n",
    "\n",
    "**Topic:** [Insert identified topic here]\n",
    "**Content:** [Insert detailed content summary for this topic here]\n",
    "\n",
    "Text Segment:\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "Ensure your output is clear, concise, and strictly adheres to the format above.\n",
    "\"\"\"\n",
    "        # print(f\"Summarizing chunk (lang: {'CH' if is_chinese_prompt else 'EN'})...\")\n",
    "        summary = self.call_ai(prompt)\n",
    "        if summary and not (summary.startswith(\"API_ERROR:\") or summary.startswith(\"REQUEST_EXCEPTION:\") or summary.startswith(\"AI_CALL_EXCEPTION:\")):\n",
    "            return summary.strip()\n",
    "        else:\n",
    "            # print(f\"Failed to summarize chunk or AI returned error: {summary}\")\n",
    "            error_msg_key = \"ä¸»é¢˜ï¼šåˆ†å—æ‘˜è¦å¤±è´¥\" if is_chinese_prompt else \"**Topic:** Chunk Summary Failed\"\n",
    "            error_msg_val = f\"æœªèƒ½å¤„ç†æ­¤æ–‡æœ¬å—ã€‚é”™è¯¯ï¼š{summary if summary else 'æœªçŸ¥é”™è¯¯'}\" if is_chinese_prompt else f\"Could not process this text block. Error: {summary if summary else 'Unknown error'}\"\n",
    "            return f\"{error_msg_key}\\n**å…·ä½“å†…å®¹ï¼š** {error_msg_val}\"\n",
    "\n",
    "\n",
    "    def _consolidate_summaries_in_language(self, combined_summaries: str, is_chinese_prompt: bool) -> str:\n",
    "        \"\"\"Consolidates and deduplicates combined chunk summaries.\"\"\"\n",
    "        if is_chinese_prompt:\n",
    "            prompt = f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¼–è¾‘ï¼Œæ“…é•¿æ•´åˆå’Œæç‚¼ä¿¡æ¯ã€‚\n",
    "ä»¥ä¸‹æ˜¯å¤šä¸ªæ–‡æœ¬ç‰‡æ®µçš„åˆæ­¥æ¦‚æ‹¬ï¼Œæ¯ä¸ªæ¦‚æ‹¬éƒ½åŒ…å«â€œä¸»é¢˜â€å’Œâ€œå…·ä½“å†…å®¹â€ã€‚\n",
    "ä½ çš„ä»»åŠ¡æ˜¯ï¼š\n",
    "1. ä»”ç»†é˜…è¯»æ‰€æœ‰æ¦‚æ‹¬ã€‚\n",
    "2. è¯†åˆ«å¹¶åˆå¹¶é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„ä¸»é¢˜ã€‚\n",
    "3. å¯¹æ¯ä¸ªç‹¬ç‰¹æˆ–åˆå¹¶åçš„ä¸»é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªæœ€ç»ˆçš„ã€ç²¾ç‚¼çš„æ€»ç»“ã€‚\n",
    "4. æœ€ç»ˆè¾“å‡ºå¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹Markdownæ ¼å¼ï¼Œæ¯ä¸ªä¸»é¢˜å ä¸€ç»„ï¼š\n",
    "\n",
    "### ä¸»é¢˜å¥1\n",
    "å…·ä½“å†…å®¹1\n",
    "\n",
    "### ä¸»é¢˜å¥2\n",
    "å…·ä½“å†…å®¹2\n",
    "...\n",
    "\n",
    "è¯·ç¡®ä¿æœ€ç»ˆè¾“å‡ºæµç•…è‡ªç„¶ï¼Œé€»è¾‘æ¸…æ™°ï¼Œå¹¶ä¸”åªåŒ…å«ç¬¦åˆä¸Šè¿°æ ¼å¼çš„Markdownå†…å®¹ã€‚ä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å‰ç¼€ã€åç¼€ã€å¼•è¨€æˆ–è§£é‡Šæ€§æ–‡å­—ã€‚\n",
    "\n",
    "å¾…æ•´åˆçš„æ¦‚æ‹¬å†…å®¹å¦‚ä¸‹ï¼š\n",
    "---\n",
    "{combined_summaries}\n",
    "---\n",
    "è¯·å¼€å§‹æ•´åˆå’Œæç‚¼ã€‚\n",
    "\"\"\"\n",
    "        else: # English prompt\n",
    "            prompt = f\"\"\"\n",
    "You are an expert editor skilled in synthesizing and refining information.\n",
    "Below are preliminary summaries from multiple text segments, each containing a \"Topic\" and \"Content\".\n",
    "Your tasks are to:\n",
    "1. Carefully read all the summaries.\n",
    "2. Identify and merge duplicate or highly similar topics.\n",
    "3. For each unique or merged topic, generate a final, concise summary.\n",
    "4. The final output must strictly follow this Markdown format, with each topic in its own group:\n",
    "\n",
    "### Topic Sentence 1\n",
    "Detailed content for topic 1\n",
    "\n",
    "### Topic Sentence 2\n",
    "Detailed content for topic 2\n",
    "...\n",
    "\n",
    "Ensure the final output is fluent, logical, and contains ONLY Markdown content adhering to the format above. Do not add any prefixes, suffixes, introductions, or explanatory text.\n",
    "\n",
    "Summaries to consolidate:\n",
    "---\n",
    "{combined_summaries}\n",
    "---\n",
    "Please begin consolidation and refinement.\n",
    "\"\"\"\n",
    "        # print(f\"Consolidating summaries (lang: {'CH' if is_chinese_prompt else 'EN'})...\")\n",
    "        # Use higher max_tokens for consolidation\n",
    "        final_result = self.call_ai(prompt, parameters={\"max_tokens\": 4000}) # qwen-turbo max is 8k tokens for context\n",
    "        if final_result and not (final_result.startswith(\"API_ERROR:\") or final_result.startswith(\"REQUEST_EXCEPTION:\") or final_result.startswith(\"AI_CALL_EXCEPTION:\")):\n",
    "            return final_result.strip()\n",
    "        else:\n",
    "            # print(f\"Failed to consolidate summaries or AI returned error: {final_result}\")\n",
    "            return f\"### æ•´åˆå¤±è´¥\\næ— æ³•å®Œæˆæœ€ç»ˆæ‘˜è¦æ•´åˆã€‚åŸå§‹åˆå¹¶æ‘˜è¦ï¼š\\n{combined_summaries}\\né”™è¯¯ï¼š{final_result if final_result else 'æœªçŸ¥é”™è¯¯'}\" \\\n",
    "                if is_chinese_prompt \\\n",
    "                else f\"### Consolidation Failed\\nCould not complete final summary consolidation. Original combined summaries:\\n{combined_summaries}\\nError: {final_result if final_result else 'Unknown error'}\"\n",
    "\n",
    "\n",
    "    def analyze(self, meeting_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Analyzes meeting text by splitting, summarizing chunks, and consolidating.\n",
    "        Returns a Markdown string.\n",
    "        \"\"\"\n",
    "        if not meeting_text or not meeting_text.strip():\n",
    "            return \"### é”™è¯¯\\nè¾“å…¥çš„ä¼šè®®å†…å®¹ä¸ºç©ºã€‚\"\n",
    "\n",
    "        language = self.detect_language(meeting_text)\n",
    "        is_chinese = (language == \"ä¸­æ–‡\")\n",
    "        # print(f\"ğŸŒ Detected language: {language}\")\n",
    "\n",
    "        # 1. Split text into chunks\n",
    "        # Word count can be tricky for CJK languages. 3000 words might be ~6000-9000 characters.\n",
    "        # qwen-turbo has a context window of 8k tokens. Let's use a character limit for splitting\n",
    "        # to be safer, or stick to word count and hope the model handles tokenization well.\n",
    "        # For words, 3000 words is a good number.\n",
    "        chunks = self._split_text_into_chunks(meeting_text, max_words=2800) # Slightly less than 3000 for safety margin\n",
    "\n",
    "        if not chunks:\n",
    "            return \"### é”™è¯¯\\næ— æ³•å°†æ–‡æœ¬åˆ†å‰²æˆå—ã€‚\" if is_chinese else \"### Error\\nCould not split text into chunks.\"\n",
    "\n",
    "        # 2. Summarize each chunk\n",
    "        chunk_summaries = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # print(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
    "            summary = self._summarize_chunk_in_language(chunk, is_chinese)\n",
    "            chunk_summaries.append(summary)\n",
    "        \n",
    "        combined_chunk_summaries = \"\\n\\n---\\n\\n\".join(chunk_summaries) # Separator for AI to distinguish\n",
    "\n",
    "        # 3. Consolidate and deduplicate summaries\n",
    "        final_markdown_summary = self._consolidate_summaries_in_language(combined_chunk_summaries, is_chinese)\n",
    "        \n",
    "        return final_markdown_summary\n",
    "\n",
    "\n",
    "def analyze_meeting_text_v2(meeting_content: str, api_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Main function to analyze meeting text (V2: Markdown multi-stage).\n",
    "    \n",
    "    Args:\n",
    "        meeting_content: The raw text of the meeting.\n",
    "        api_key: Your Dashscope API key.\n",
    "        \n",
    "    Returns:\n",
    "        A Markdown string of the analysis result, or an error message.\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        return \"### é”™è¯¯\\nAPIå¯†é’¥æ˜¯å¿…éœ€çš„ã€‚\"\n",
    "    if not meeting_content or not meeting_content.strip():\n",
    "        return \"### é”™è¯¯\\nä¼šè®®å†…å®¹ä¸èƒ½ä¸ºç©ºã€‚\"\n",
    "\n",
    "    analyzer = MeetingAnalyzerV2(api_key)\n",
    "    analysis_result_markdown = analyzer.analyze(meeting_content)\n",
    "    \n",
    "    return analysis_result_markdown\n",
    "\n",
    "# Example Usage (main part for testing):\n",
    "if __name__ == \"__main__\":\n",
    "    # IMPORTANT: Set your API key here or via an environment variable\n",
    "    try:\n",
    "        API_KEY = open('qwen.key').read().strip() \n",
    "        if not API_KEY or \"your_actual_api_key_here\" in API_KEY: # Basic check\n",
    "            raise FileNotFoundError \n",
    "    except FileNotFoundError:\n",
    "        API_KEY = \"sk-your_actual_api_key_here\" # Placeholder\n",
    "        print(\"ğŸš¨ API key file 'qwen.key' not found or empty. Please create it with your Dashscope API key.\")\n",
    "\n",
    "    if \"your_actual_api_key_here\" in API_KEY:\n",
    "        print(\"ğŸš¨ Please replace 'sk-your_actual_api_key_here' with your actual Dashscope API key to run the example.\")\n",
    "    else:\n",
    "        print(\"ğŸš€ Meeting Analysis Tool (V2 - Markdown Output)\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Test with a sample English text file\n",
    "        try:\n",
    "            # Use a longer text file for meaningful chunking\n",
    "            # Create a dummy long_english_text.txt if you don't have one\n",
    "            # For example, repeat the content of 'huggingcast-s2e6 text-en.txt' multiple times\n",
    "            with open('huggingcast-s2e6 text-en.txt', 'r', encoding='utf-8') as f_en:\n",
    "                english_meeting_text_short = f_en.read()\n",
    "\n",
    "            print(\"\\n--- Analyzing English Meeting Text (expecting multiple chunks) ---\")\n",
    "            result_markdown_en = analyze_meeting_text_v2(english_meeting_text_short, API_KEY)\n",
    "            print(\"\\nAnalysis Result (Markdown String - English):\")\n",
    "            print(result_markdown_en)\n",
    "\n",
    "            # You can add a Chinese text example similarly if you have one\n",
    "            # chinese_meeting_text = \"\"\"\n",
    "            # [ä¸€æ®µè¾ƒé•¿çš„ä¸­æ–‡ä¼šè®®è®°å½•ï¼Œæœ€å¥½è¶…è¿‡3000ä¸ªâ€œè¯â€çš„æ¦‚å¿µï¼Œå¯èƒ½éœ€è¦å‡ åƒåˆ°ä¸€ä¸‡ä¸ªæ±‰å­—]\n",
    "            # ç¬¬ä¸€æ¬¡é¡¹ç›®å¯åŠ¨ä¼šè®¨è®ºäº†å…³äºæ–°äº§å“â€œæ™ºèƒ½åŠ©æ‰‹â€çš„å¼€å‘è®¡åˆ’ã€‚\n",
    "            # å¼ ä¸‰ï¼šæˆ‘è®¤ä¸ºæˆ‘ä»¬é¦–å…ˆéœ€è¦æ˜ç¡®äº§å“çš„æ ¸å¿ƒåŠŸèƒ½ã€‚æ˜¯ä¸»æ‰“è¯­éŸ³äº¤äº’è¿˜æ˜¯æ–‡æœ¬è¾…åŠ©ï¼Ÿ\n",
    "            # æå››ï¼šæˆ‘åŒæ„å¼ ä¸‰çš„çœ‹æ³•ã€‚å¸‚åœºè°ƒç ”æ˜¾ç¤ºï¼Œç”¨æˆ·å¯¹é«˜æ•ˆè¯­éŸ³äº¤äº’çš„éœ€æ±‚å¾ˆé«˜ã€‚å»ºè®®åˆæœŸé›†ä¸­èµ„æºæ”»å…‹è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€ç†è§£çš„å‡†ç¡®æ€§ã€‚\n",
    "            # ç‹äº”ï¼šå…³äºæ—¶é—´èŠ‚ç‚¹ï¼Œæˆ‘ä»¬èƒ½å¦åœ¨ä¸‰ä¸ªæœˆå†…æ¨å‡ºç¬¬ä¸€ä¸ªbetaç‰ˆæœ¬ï¼Ÿ\n",
    "            # èµµå…­ï¼šä¸‰ä¸ªæœˆæ¯”è¾ƒç´§å¼ ï¼Œè€ƒè™‘åˆ°æŠ€æœ¯éš¾ç‚¹å’Œå›¢é˜Ÿç£¨åˆï¼Œæˆ‘å»ºè®®å››ä¸ªæœˆï¼Œç¡®ä¿è´¨é‡ã€‚\n",
    "            # ... (æ›´å¤šå†…å®¹ï¼Œç¡®ä¿æ–‡æœ¬è¶³å¤Ÿé•¿) ...\n",
    "            # ç¬¬äºŒéƒ¨åˆ†è®¨è®ºäº†å¸‚åœºæ¨å¹¿ç­–ç•¥ã€‚\n",
    "            # é™ˆä¸ƒï¼šæ—©æœŸç”¨æˆ·è·å–æ–¹é¢ï¼Œæˆ‘ä»¬è€ƒè™‘å’Œä¸€äº›ç§‘æŠ€KOLåˆä½œã€‚\n",
    "            # å‘¨å…«ï¼šç¤¾äº¤åª’ä½“å®£ä¼ ä¹Ÿå¾ˆé‡è¦ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¹´è½»ç”¨æˆ·ç¾¤ä½“ã€‚\n",
    "            # ... (æ›´å¤šå†…å®¹) ...\n",
    "            # æœ€ç»ˆå†³å®šï¼Œäº§å“æ ¸å¿ƒåŠŸèƒ½ä¾§é‡è¯­éŸ³äº¤äº’ï¼Œå¼€å‘å‘¨æœŸå®šä¸ºå››ä¸ªæœˆï¼Œå¸‚åœºæ¨å¹¿åˆæœŸä»¥KOLåˆä½œä¸ºä¸»ã€‚\n",
    "            # ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼šæå››è´Ÿè´£æ•´ç†è¯¦ç»†çš„æŠ€æœ¯éœ€æ±‚æ–‡æ¡£ï¼Œä¸‹å‘¨ä¸€å‰å®Œæˆã€‚ç‹äº”å’Œèµµå…­å…±åŒåˆ¶å®šè¯¦ç»†çš„é¡¹ç›®æ’æœŸã€‚é™ˆä¸ƒè°ƒç ”åˆé€‚çš„KOLèµ„æºã€‚\n",
    "            # \"\"\"\n",
    "            # print(\"\\n--- Analyzing Chinese Meeting Text ---\")\n",
    "            # result_markdown_ch = analyze_meeting_text_v2(chinese_meeting_text * 5, API_KEY) # Multiply to make it long\n",
    "            # print(\"\\nAnalysis Result (Markdown String - Chinese):\")\n",
    "            # print(result_markdown_ch)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"\\nâš ï¸ English test file 'huggingcast-s2e6 text-en.txt' not found. Skipping English example.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ An error occurred during testing: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

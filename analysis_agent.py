import json
import requests
import re
# os and pathlib are not needed
# from datetime import datetime # Not needed

class MeetingAnalyzerV2:
    def __init__(self):
        self.api_key = open('qwen.key', 'r').read()
        self.api_url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
        # Increased max_tokens as summaries can be long, especially combined ones
        self.default_parameters = {"temperature": 0.3, "max_tokens": 3500} 

    def detect_language(self, text):
        # Simple language detection based on character counts
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        
        if chinese_chars > english_chars:
            return "ä¸­æ–‡"
        elif english_chars > chinese_chars and english_chars > 20:
            return "è‹±æ–‡"
        elif chinese_chars > 0 :
            return "ä¸­æ–‡"
        else:
            return "è‹±æ–‡"

    def call_ai(self, prompt, parameters=None):
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        current_parameters = self.default_parameters.copy()
        if parameters:
            current_parameters.update(parameters)
            
        data = {
            "model": "qwen-turbo", # or qwen-long for very long contexts if needed/available
            "input": {"messages": [{"role": "user", "content": prompt}]},
            "parameters": current_parameters
        }
        
        try:
            # print(f"ğŸ”„ Calling AI with prompt (first 100 chars): {prompt[:100]}...")
            resp = requests.post(self.api_url, headers=headers, json=data, timeout=120) # Increased timeout
            # print(f"ğŸ“¡ API Status: {resp.status_code}")
            
            if resp.status_code == 200:
                result = resp.json()
                if 'output' in result:
                    if 'choices' in result['output'] and result['output']['choices']:
                        return result['output']['choices'][0]['message']['content']
                    elif 'text' in result['output']:
                        return result['output']['text']
                print(f"âš ï¸ Unexpected API response structure: {result}")
                return None
            else:
                print(f"âŒ API Error: {resp.status_code} - {resp.text}")
                return f"API_ERROR: {resp.status_code} - {resp.text}" # Return error string
        except requests.exceptions.RequestException as e:
            print(f"âŒ Request Exception: {str(e)}")
            return f"REQUEST_EXCEPTION: {str(e)}"
        except Exception as e:
            print(f"âŒ AI Call Exception: {str(e)}")
            return f"AI_CALL_EXCEPTION: {str(e)}"
        return None


    def _split_text_into_chunks(self, text: str, max_words: int = 3000) -> list[str]:
        """Splits text into chunks of approximately max_words."""
        words = text.split() # Simple split by whitespace
        chunks = []
        for i in range(0, len(words), max_words):
            chunk_words = words[i:i + max_words]
            chunks.append(" ".join(chunk_words))
        # print(f"Split text into {len(chunks)} chunks.")
        return chunks

    def _summarize_chunk_in_language(self, chunk_text: str, is_chinese_prompt: bool) -> str:
        """Summarizes a single chunk of text using Markdown format."""
        if is_chinese_prompt:
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¼šè®®çºªè¦åŠ©æ‰‹ï¼Œè¯·ä½ æ ¹æ®ä»¥ä¸‹æ¨¡ç‰ˆç”Ÿæˆä¸€ä¸ªä¼šè®®çºªè¦
            
ä¼šè®®çºªè¦ (Meeting Minutes)

ä¼šè®®åç§°ï¼š[æ ¹æ®è¾“å…¥å¡«å†™]

ä¸€ã€ä¼šè®®è®®é¢˜ä¸è®¨è®ºæ‘˜è¦ï¼š

    1.  è®®é¢˜ä¸€ï¼š[è®®é¢˜åç§°]
        *   ä¸»è¦è®¨è®ºç‚¹ï¼š
            *   [è®¨è®ºç‚¹Açš„ç®€è¿°]
            *   [è®¨è®ºç‚¹Bçš„ç®€è¿°]
            *   [ä¸åŒè§‚ç‚¹æˆ–é‡è¦å‘è¨€æ‘˜è¦]
        *   å†³è®®ï¼š
            *   [å†³è®®1çš„å†…å®¹]
        *   è¡ŒåŠ¨é¡¹ï¼š
            *   [è´Ÿè´£äººA]ï¼š[å…·ä½“ä»»åŠ¡A] - [æˆªæ­¢æ—¥æœŸA]

    2.  è®®é¢˜äºŒï¼š[è®®é¢˜åç§°]
        *   ä¸»è¦è®¨è®ºç‚¹ï¼š
            *   ...
        *   å†³è®®ï¼š
            *   ...
        *   è¡ŒåŠ¨é¡¹ï¼š
            *   ...

    (æ ¹æ®å®é™…è®®é¢˜æ•°é‡å¢å‡)

äºŒã€æ€»ç»“ä¸å…³é”®å†³ç­–ï¼š
    *   [å¯¹ä¼šè®®è¾¾æˆçš„ä¸»è¦å…±è¯†æˆ–é‡è¦å†³ç­–è¿›è¡Œæ€»ç»“]

ä¸‰ã€è¡ŒåŠ¨è®¡åˆ’ä¸åˆ†é… (Action Items Summary):
    *   è¡ŒåŠ¨é¡¹1ï¼š
        *   ä»»åŠ¡æè¿°ï¼š[å…·ä½“ä»»åŠ¡]
        *   è´Ÿè´£äººï¼š[å§“å]
        *   æˆªæ­¢æ—¥æœŸï¼š[YYYY-MM-DD]
    *   è¡ŒåŠ¨é¡¹2ï¼š
        *   ä»»åŠ¡æè¿°ï¼š[å…·ä½“ä»»åŠ¡]
        *   è´Ÿè´£äººï¼š[å§“å]
        *   æˆªæ­¢æ—¥æœŸï¼š[YYYY-MM-DD]
    *   ...

å››ã€å…¶ä»–äº‹é¡¹ (Any Other Business - AOB)ï¼š
    *   [è®°å½•è®¨è®ºåˆ°çš„å…¶ä»–æœªåˆ—å…¥è®®ç¨‹ä½†é‡è¦äº‹é¡¹]
    
- Original text content: {chunk_text}
"""
        else: # English prompt
            prompt = f"""
Meeting Minutes

Meeting Title: [Based on input]


I. Agenda Items & Discussion Summary:

    1.  Agenda Item 1: [Item Title]
        *   Key Discussion Points:
            *   [Brief description of discussion point A]
            *   [Brief description of discussion point B]
            *   [Summary of different opinions or important statements]
        *   Decisions:
            *   [Content of decision 1]
        *   Action Items:
            *   [Owner A]: [Specific Task A] - [Due Date A]

    2.  Agenda Item 2: [Item Title]
        *   Key Discussion Points:
            *   ...
        *   Decisions:
            *   ...
        *   Action Items:
            *   ...

    (Add or remove sections based on the number of agenda items)

II. Summary & Key Decisions:
    *   [Summarize the main consensus or important decisions reached during the meeting]

III. Action Items Summary & Assignments:
    *   Action Item 1:
        *   Task Description: [Specific Task]
        *   Responsible: [Name]
        *   Due Date: [YYYY-MM-DD]
    *   Action Item 2:
        *   Task Description: [Specific Task]
        *   Responsible: [Name]
        *   Due Date: [YYYY-MM-DD]
    *   ...

IV. Any Other Business (AOB):
    *   [Record any other important matters discussed that were not on the agenda]
    
- Original text content: {chunk_text}
"""
        # print(f"Summarizing chunk (lang: {'CH' if is_chinese_prompt else 'EN'})...")
        summary = self.call_ai(prompt)
        if summary and not (summary.startswith("API_ERROR:") or summary.startswith("REQUEST_EXCEPTION:") or summary.startswith("AI_CALL_EXCEPTION:")):
            return summary.strip()
        else:
            # print(f"Failed to summarize chunk or AI returned error: {summary}")
            error_msg_key = "ä¸»é¢˜ï¼šåˆ†å—æ‘˜è¦å¤±è´¥" if is_chinese_prompt else "**Topic:** Chunk Summary Failed"
            error_msg_val = f"æœªèƒ½å¤„ç†æ­¤æ–‡æœ¬å—ã€‚é”™è¯¯ï¼š{summary if summary else 'æœªçŸ¥é”™è¯¯'}" if is_chinese_prompt else f"Could not process this text block. Error: {summary if summary else 'Unknown error'}"
            return f"{error_msg_key}\n**å…·ä½“å†…å®¹ï¼š** {error_msg_val}"


    def _consolidate_summaries_in_language(self, combined_summaries: str, is_chinese_prompt: bool) -> str:
        """Consolidates and deduplicates combined chunk summaries."""
        if is_chinese_prompt:
            prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¼–è¾‘ï¼Œæ“…é•¿æ•´åˆå’Œæç‚¼ä¿¡æ¯ã€‚
ä»¥ä¸‹æ˜¯å¤šä¸ªæ–‡æœ¬ç‰‡æ®µçš„åˆæ­¥æ¦‚æ‹¬ï¼Œæ¯ä¸ªæ¦‚æ‹¬éƒ½åŒ…å«â€œä¸»é¢˜â€å’Œâ€œå…·ä½“å†…å®¹â€ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ä»”ç»†é˜…è¯»æ‰€æœ‰æ¦‚æ‹¬ã€‚
2. è¯†åˆ«å¹¶åˆå¹¶é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„ä¸»é¢˜ã€‚
3. å¯¹æ¯ä¸ªç‹¬ç‰¹æˆ–åˆå¹¶åçš„ä¸»é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªæœ€ç»ˆçš„ã€ç²¾ç‚¼çš„æ€»ç»“ã€‚


å¾…æ•´åˆçš„æ¦‚æ‹¬å†…å®¹å¦‚ä¸‹ï¼š
---
{combined_summaries}
---
è¯·å¼€å§‹æ•´åˆå’Œæç‚¼ã€‚
"""
        else: # English prompt
            prompt = f"""
You are an expert editor skilled in synthesizing and refining information.
Below are preliminary summaries from multiple text segments, each containing a "Topic" and "Content".
Your tasks are to:
1. Carefully read all the summaries.
2. Identify and merge duplicate or highly similar topics.
3. For each unique or merged topic, generate a final, concise summary.

Summaries to consolidate:
---
{combined_summaries}
---
Please begin consolidation and refinement.
"""
        # print(f"Consolidating summaries (lang: {'CH' if is_chinese_prompt else 'EN'})...")
        # Use higher max_tokens for consolidation
        final_result = self.call_ai(prompt, parameters={"max_tokens": 8000}) # qwen-turbo max is 8k tokens for context
        if final_result and not (final_result.startswith("API_ERROR:") or final_result.startswith("REQUEST_EXCEPTION:") or final_result.startswith("AI_CALL_EXCEPTION:")):
            return final_result.strip()
        else:
            # print(f"Failed to consolidate summaries or AI returned error: {final_result}")
            return f"### æ•´åˆå¤±è´¥\næ— æ³•å®Œæˆæœ€ç»ˆæ‘˜è¦æ•´åˆã€‚åŸå§‹åˆå¹¶æ‘˜è¦ï¼š\n{combined_summaries}\né”™è¯¯ï¼š{final_result if final_result else 'æœªçŸ¥é”™è¯¯'}" \
                if is_chinese_prompt \
                else f"### Consolidation Failed\nCould not complete final summary consolidation. Original combined summaries:\n{combined_summaries}\nError: {final_result if final_result else 'Unknown error'}"


    def analyze(self, meeting_text: str) -> str:
        """
        Analyzes meeting text by splitting, summarizing chunks, and consolidating.
        Returns a Markdown string.
        """
        if not meeting_text or not meeting_text.strip():
            return "### é”™è¯¯\nè¾“å…¥çš„ä¼šè®®å†…å®¹ä¸ºç©ºã€‚"

        language = self.detect_language(meeting_text)
        is_chinese = (language == "ä¸­æ–‡")
        # print(f"ğŸŒ Detected language: {language}")

        # 1. Split text into chunks
        # Word count can be tricky for CJK languages. 3000 words might be ~6000-9000 characters.
        # qwen-turbo has a context window of 8k tokens. Let's use a character limit for splitting
        # to be safer, or stick to word count and hope the model handles tokenization well.
        # For words, 3000 words is a good number.
        chunks = self._split_text_into_chunks(meeting_text, max_words=2800) # Slightly less than 3000 for safety margin

        if not chunks:
            return "### é”™è¯¯\næ— æ³•å°†æ–‡æœ¬åˆ†å‰²æˆå—ã€‚" if is_chinese else "### Error\nCould not split text into chunks."

        # 2. Summarize each chunk
        chunk_summaries = []
        for i, chunk in enumerate(chunks):
            # print(f"Processing chunk {i+1}/{len(chunks)}")
            summary = self._summarize_chunk_in_language(chunk, is_chinese)
            chunk_summaries.append(summary)
        
        combined_chunk_summaries = "\n\n---\n\n".join(chunk_summaries) # Separator for AI to distinguish

        # 3. Consolidate and deduplicate summaries
        final_markdown_summary = self._consolidate_summaries_in_language(combined_chunk_summaries, is_chinese)
        
        return final_markdown_summary


def analyze_meeting_text_v2(meeting_content: str) -> str:
    """
    Main function to analyze meeting text (V2: Markdown multi-stage).
    
    Args:
        meeting_content: The raw text of the meeting.
        api_key: Your Dashscope API key.
        
    Returns:
        A Markdown string of the analysis result, or an error message.
    """
    if not meeting_content or not meeting_content.strip():
        return "### é”™è¯¯\nä¼šè®®å†…å®¹ä¸èƒ½ä¸ºç©ºã€‚"

    analyzer = MeetingAnalyzerV2()
    analysis_result_markdown = analyzer.analyze(meeting_content)
    
    return analysis_result_markdown

if __name__ == "__main__":
    analyzer = MeetingAnalyzerV2()
    analysis_result_markdown = analyzer.analyze(open('/home/dell/mwy/AIML-project/sora.txt', 'r').read())
    print(analysis_result_markdown)
    
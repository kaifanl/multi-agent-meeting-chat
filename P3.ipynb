{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6518dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 会议分析助手启动\n",
      "🎉 欢迎使用会议分析助手！\n",
      "==================================================\n",
      "\n",
      "📝 请选择输入方式：\n",
      "1. 直接粘贴会议内容\n",
      "2. 上传文件 (支持 .txt, .docx)\n",
      "3. 退出\n",
      "\n",
      "==================================================\n",
      "📋 请粘贴会议内容 (粘贴完成后按回车):\n",
      "==================================================\n",
      "✅ 已获取 39785 字符的会议内容\n",
      "\n",
      "🔍 正在分析会议内容...\n",
      "🌐 检测语言: 英文\n",
      "🔄 分析英文会议...\n",
      "⚠️ 会议内容较长(39785字符)，将分析前8000字符\n",
      "🔄 正在调用AI...\n",
      "📡 API响应状态: 200\n",
      "🔍 API返回结构: ['output', 'usage', 'request_id']\n",
      "📋 output结构: ['finish_reason', 'text']\n",
      "\n",
      "==================================================\n",
      "📋 English Meeting Analysis\n",
      "==================================================\n",
      "📝 Meeting Summary: The meeting discussed Hugging Face's collaboration with Intel to showcase efficient AI model building using Intel's CPUs and AI accelerators. The focus was on demonstrating how to optimize workloads with Optimum Intel and Gaudi 3, emphasizing scalability and performance improvements.\n",
      "\n",
      "🎯 Key Topics:\n",
      "  • Hugging Cast Overview (high priority)\n",
      "    - Prepare upcoming episodes → Unclear\n",
      "    - Engage with live audience during demos → Hosts\n",
      "  • Optimization with Optimum Intel (high priority)\n",
      "    - Develop tutorials for Optimum Intel → Documentation Team\n",
      "    - Test integration with popular models → Core Maintainers\n",
      "  • Intel Gaudi 3 Accelerator (high priority)\n",
      "    - Launch Gaudi 3 demo environment → Reis\n",
      "    - Create user guides for Gaudi 3 → Technical Writers\n",
      "\n",
      "✅ Decisions:\n",
      "  • Decide to prioritize CPU optimization alongside GPU workflows.\n",
      "  • Choose to highlight Optimum Intel as the primary tool for Intel platform optimization.\n",
      "\n",
      "⏭️ Next Steps:\n",
      "  • Launch Gaudi 3 demo environment\n",
      "  • Publish blog post on Intel Gaudi 3 optimizations\n",
      "  • Develop tutorial series for Optimum Intel\n",
      "\n",
      "❓ Unresolved Issues:\n",
      "  • Clarification on specific assignees for tasks\n",
      "  • Exact deadlines for certain deliverables\n",
      "\n",
      "==================================================\n",
      "==================================================\n",
      "🔄 翻译中...\n",
      "🔄 正在调用AI...\n",
      "📡 API响应状态: 200\n",
      "🔍 API返回结构: ['output', 'usage', 'request_id']\n",
      "📋 output结构: ['finish_reason', 'text']\n",
      "✅ 翻译完成！\n",
      "\n",
      "==================================================\n",
      "📋 中文会议纪要\n",
      "==================================================\n",
      "📝 会议总结：会议讨论了Hugging Face与Intel的合作，展示了如何使用Intel的CPU和AI加速器高效构建AI模型。重点在于展示如何通过Optimum Intel和Gaudi 3优化工作负载，强调可扩展性和性能提升。\n",
      "\n",
      "🎯 关键议题：\n",
      "  • Hugging Cast概述 (重要性：高)\n",
      "    - 准备即将播出的剧集 → 不明确\n",
      "    - 在演示期间与现场观众互动 → 主持人\n",
      "  • 使用Optimum Intel进行优化 (重要性：高)\n",
      "    - 开发Optimum Intel教程 → 文档团队\n",
      "    - 测试与流行模型的集成 → 核心维护人员\n",
      "  • Intel Gaudi 3加速器 (重要性：高)\n",
      "    - 启动Gaudi 3演示环境 → Reis\n",
      "    - 创建Gaudi 3用户指南 → 技术作家\n",
      "\n",
      "✅ 决策事项：\n",
      "  • 决定优先考虑CPU优化与GPU工作流并行发展。\n",
      "  • 选择将Optimum Intel作为Intel平台优化的主要工具。\n",
      "\n",
      "⏭️ 下一步计划：\n",
      "  • 启动Gaudi 3演示环境\n",
      "  • 发布有关Intel Gaudi 3优化的博客文章\n",
      "  • 开发Optimum Intel教程系列\n",
      "\n",
      "❓ 未解决问题：\n",
      "  • 任务具体负责人的澄清\n",
      "  • 某些交付物的具体截止日期\n",
      "\n",
      "🎯 分析完成！\n",
      "❌ 保存失败: [Errno 30] Read-only file system: 'meeting_analysis_20250529_202705.json'\n",
      "\n",
      "📝 请选择输入方式：\n",
      "1. 直接粘贴会议内容\n",
      "2. 上传文件 (支持 .txt, .docx)\n",
      "3. 退出\n",
      "\n",
      "==================================================\n",
      "📋 请粘贴会议内容 (粘贴完成后按回车):\n",
      "==================================================\n",
      "❌ 没有输入内容\n",
      "\n",
      "📝 请选择输入方式：\n",
      "1. 直接粘贴会议内容\n",
      "2. 上传文件 (支持 .txt, .docx)\n",
      "3. 退出\n",
      "👋 再见！\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class MeetingHelper:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\"\n",
    "    \n",
    "    def start(self):\n",
    "        print(\"🎉 欢迎使用会议分析助手！\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\n📝 请选择输入方式：\")\n",
    "            print(\"1. 直接粘贴会议内容\")\n",
    "            print(\"2. 上传文件 (支持 .txt, .docx)\")\n",
    "            print(\"3. 退出\")\n",
    "            \n",
    "            choice = input(\"\\n请选择 (1-3): \").strip()\n",
    "            \n",
    "            if choice == \"1\":\n",
    "                meeting_text = self.get_text_input()\n",
    "                if meeting_text:\n",
    "                    self.process_meeting(meeting_text)\n",
    "            elif choice == \"2\":\n",
    "                meeting_text = self.get_file_input()\n",
    "                if meeting_text:\n",
    "                    self.process_meeting(meeting_text)\n",
    "            elif choice == \"3\":\n",
    "                print(\"👋 再见！\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"❌ 请选择 1-3\")\n",
    "    \n",
    "\n",
    "    \n",
    "    def process_meeting(self, meeting_text):\n",
    "        print(\"\\n🔍 正在分析会议内容...\")\n",
    "        result = self.analyze(meeting_text)\n",
    "        \n",
    "        if result[\"ok\"]:\n",
    "            print(\"\\n🎯 分析完成！\")\n",
    "            save_choice = input(\"\\n💾 是否保存分析结果到文件？(y/n): \").lower().strip()\n",
    "            if save_choice in ['y', 'yes', '是', '好']:\n",
    "                self.save_result(result[\"result\"])\n",
    "        else:\n",
    "            print(f\"❌ 分析失败: {result['error']}\")\n",
    "    \n",
    "    def save_result(self, result):\n",
    "        try:\n",
    "            filename = f\"meeting_analysis_{self.get_timestamp()}.json\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"✅ 结果已保存到: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 保存失败: {str(e)}\")\n",
    "    \n",
    "    def get_timestamp(self):\n",
    "        from datetime import datetime\n",
    "        return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    def detect_language(self, text):\n",
    "        chinese_chars = len(re.findall(r'[\\u4e00-\\u9fff]', text))\n",
    "        english_chars = len(re.findall(r'[a-zA-Z]', text))\n",
    "        \n",
    "        if chinese_chars > english_chars:\n",
    "            return \"中文\"\n",
    "        elif english_chars > chinese_chars * 2:\n",
    "            return \"英文\"\n",
    "        else:\n",
    "            return \"混合\"\n",
    "    \n",
    "    def analyze(self, meeting_text, auto_translate=False):\n",
    "        language = self.detect_language(meeting_text)\n",
    "        print(f\"🌐 检测语言: {language}\")\n",
    "        \n",
    "        if language == \"英文\":\n",
    "            print(\"🔄 分析英文会议...\")\n",
    "            english_result = self.analyze_in_english(meeting_text)\n",
    "            \n",
    "            if english_result[\"ok\"]:\n",
    "                if not auto_translate:\n",
    "                    print(\"\\n\" + \"=\"*50)\n",
    "                    print(\"📋 English Meeting Analysis\")\n",
    "                    print(\"=\"*50)\n",
    "                    self.print_result_english(english_result[\"result\"])\n",
    "                    \n",
    "                    print(\"\\n\" + \"=\"*50)\n",
    "                    translate_choice = input(\"🤔 需要翻译成中文吗？(y/n): \").lower().strip()\n",
    "                    print(\"=\"*50)\n",
    "                    \n",
    "                    if translate_choice in ['y', 'yes', '是', '好']:\n",
    "                        print(\"🔄 翻译中...\")\n",
    "                        chinese_result = self.translate_result_to_chinese(english_result[\"result\"])\n",
    "                        if chinese_result[\"ok\"]:\n",
    "                            print(\"✅ 翻译完成！\")\n",
    "                            print(\"\\n\" + \"=\"*50)\n",
    "                            print(\"📋 中文会议纪要\")\n",
    "                            print(\"=\"*50)\n",
    "                            self.print_result_chinese(chinese_result[\"result\"])\n",
    "                            return chinese_result\n",
    "                        else:\n",
    "                            print(\"❌ 翻译失败，保留英文版本\")\n",
    "                            return english_result\n",
    "                    else:\n",
    "                        return english_result\n",
    "                else:\n",
    "                    chinese_result = self.translate_result_to_chinese(english_result[\"result\"])\n",
    "                    return chinese_result if chinese_result[\"ok\"] else english_result\n",
    "            else:\n",
    "                return english_result\n",
    "        else:\n",
    "            print(\"🔄 分析中文会议...\")\n",
    "            chinese_result = self.analyze_in_chinese(meeting_text)\n",
    "            \n",
    "            if chinese_result[\"ok\"]:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"📋 中文会议纪要\")\n",
    "                print(\"=\"*50)\n",
    "                self.print_result_chinese(chinese_result[\"result\"])\n",
    "            \n",
    "            return chinese_result\n",
    "    \n",
    "    def analyze_in_english(self, meeting_text):\n",
    "        if len(meeting_text) > 8000:\n",
    "            print(f\"⚠️ 会议内容较长({len(meeting_text)}字符)，将分析前8000字符\")\n",
    "            meeting_text = meeting_text[:8000]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a professional meeting minutes assistant. Please extract key information from the following meeting content and return it in JSON format.\n",
    "\n",
    "Meeting Content:\n",
    "{meeting_text}\n",
    "\n",
    "Please generate JSON in the following format:\n",
    "{{\n",
    "  \"meeting_summary\": \"Brief summary of meeting content, 2-3 sentences\",\n",
    "  \"key_topics\": [\n",
    "    {{\n",
    "      \"topic\": \"Topic name\",\n",
    "      \"description\": \"Detailed description of the topic\",\n",
    "      \"importance\": \"high/medium/low\",\n",
    "      \"related_tasks\": [\n",
    "        {{\n",
    "          \"task\": \"Task name under this topic\",\n",
    "          \"assignee\": \"Responsible person\",\n",
    "          \"deadline\": \"Deadline\",\n",
    "          \"priority\": \"high/medium/low\",\n",
    "          \"status\": \"pending/in-progress/completed\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ],\n",
    "  \"decisions\": [\n",
    "    {{\n",
    "      \"decision\": \"Decision content\",\n",
    "      \"decision_maker\": \"Decision maker\",\n",
    "      \"impact\": \"Impact\"\n",
    "    }}\n",
    "  ],\n",
    "  \"next_steps\": [\"Next steps\"],\n",
    "  \"unresolved_issues\": [\"Unresolved issues\"]\n",
    "}}\n",
    "Requirements:\n",
    "- Each topic can contain multiple subtasks.\n",
    "- If information is unclear, fill in \"unclear\".\n",
    "- Return must be strict JSON format without explanations.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.call_ai(prompt)\n",
    "            if response:\n",
    "                data = self.parse_json(response)\n",
    "                if data:\n",
    "                    return {\"ok\": True, \"result\": self.fix_data_english(data)}\n",
    "                else:\n",
    "                    return self.simple_way_english(meeting_text)\n",
    "            else:\n",
    "                return {\"ok\": False, \"error\": \"AI调用失败\"}\n",
    "        except Exception as e:\n",
    "            return {\"ok\": False, \"error\": f\"分析出错: {str(e)}\"}\n",
    "    \n",
    "    def analyze_in_chinese(self, meeting_text):\n",
    "        # 如果文本太长，先截取前面部分\n",
    "        if len(meeting_text) > 8000:\n",
    "            print(f\"⚠️ 会议内容较长({len(meeting_text)}字符)，将分析前8000字符\")\n",
    "            meeting_text = meeting_text[:8000]\n",
    "            \n",
    "        prompt = f\"\"\"\n",
    "你是一位专业的会议纪要助手，请根据以下会议内容，提取关键信息并以JSON格式返回。\n",
    "会议内容：\n",
    "{meeting_text}\n",
    "请按照以下格式生成JSON：\n",
    "{{\n",
    "  \"meeting_summary\": \"简要总结会议内容，2-3句话\",\n",
    "  \"key_topics\": [\n",
    "    {{\n",
    "      \"topic\": \"议题名称\",\n",
    "      \"description\": \"该议题的详细描述\",\n",
    "      \"importance\": \"high/medium/low\",\n",
    "      \"related_tasks\": [\n",
    "        {{\n",
    "          \"task\": \"该议题下的子任务名称\",\n",
    "          \"assignee\": \"负责人\",\n",
    "          \"deadline\": \"截止时间\",\n",
    "          \"priority\": \"high/medium/low\",\n",
    "          \"status\": \"待处理/进行中/已完成\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ],\n",
    "  \"decisions\": [\n",
    "    {{\n",
    "      \"decision\": \"决策内容\",\n",
    "      \"decision_maker\": \"决策人\",\n",
    "      \"impact\": \"影响\"\n",
    "    }}\n",
    "  ],\n",
    "  \"next_steps\": [\"下一步计划\"],\n",
    "  \"unresolved_issues\": [\"未解决问题\"]\n",
    "}}\n",
    "要求：\n",
    "- 每个议题都可以包含多个子任务。\n",
    "- 子任务字段中，如果信息不明确，请填写\"未明确\"。\n",
    "- 返回结果必须是严格的JSON格式，不要包含解释说明或多余文字。\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.call_ai(prompt)\n",
    "            if response:\n",
    "                data = self.parse_json(response)\n",
    "                if data:\n",
    "                    return {\"ok\": True, \"result\": self.fix_data_chinese(data)}\n",
    "                else:\n",
    "                    return self.simple_way_chinese(meeting_text)\n",
    "            else:\n",
    "                return {\"ok\": False, \"error\": \"AI调用失败\"}\n",
    "        except Exception as e:\n",
    "            return {\"ok\": False, \"error\": f\"分析出错: {str(e)}\"}\n",
    "    \n",
    "    def call_ai(self, prompt):\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {self.api_key}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"qwen-turbo\",\n",
    "            \"input\": {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
    "            \"parameters\": {\"temperature\": 0.3, \"max_tokens\": 2000}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(\"🔄 正在调用AI...\")\n",
    "            resp = requests.post(self.api_url, headers=headers, json=data)\n",
    "            print(f\"📡 API响应状态: {resp.status_code}\")\n",
    "            \n",
    "            if resp.status_code == 200:\n",
    "                result = resp.json()\n",
    "                print(f\"🔍 API返回结构: {list(result.keys())}\")\n",
    "                \n",
    "                # 调试：打印完整响应结构\n",
    "                if 'output' in result:\n",
    "                    print(f\"📋 output结构: {list(result['output'].keys())}\")\n",
    "                    if 'choices' in result['output']:\n",
    "                        return result['output']['choices'][0]['message']['content']\n",
    "                    elif 'text' in result['output']:\n",
    "                        return result['output']['text']\n",
    "                    else:\n",
    "                        print(f\"⚠️ 未找到预期字段，完整响应: {result}\")\n",
    "                        return None\n",
    "                else:\n",
    "                    print(f\"⚠️ 没有output字段，完整响应: {result}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"❌ API错误: {resp.status_code}\")\n",
    "                print(f\"错误内容: {resp.text}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 请求异常: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_json(self, text):\n",
    "        try:\n",
    "            clean_text = text.strip()\n",
    "            if clean_text.startswith('```json'):\n",
    "                clean_text = clean_text.split('```json')[1]\n",
    "            if clean_text.endswith('```'):\n",
    "                clean_text = clean_text.rsplit('```', 1)[0]\n",
    "            return json.loads(clean_text)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def translate_result_to_chinese(self, english_result):\n",
    "        translate_prompt = f\"\"\"\n",
    "请将以下英文会议分析结果翻译成中文，保持JSON结构不变：\n",
    "\n",
    "{json.dumps(english_result, indent=2)}\n",
    "\n",
    "要求：\n",
    "- 保持JSON格式完全一致\n",
    "- 只翻译文本内容，不改变字段名\n",
    "- 翻译要自然流畅\n",
    "- 直接返回翻译后的JSON，不要解释\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.call_ai(translate_prompt)\n",
    "            if response:\n",
    "                translated_data = self.parse_json(response)\n",
    "                if translated_data:\n",
    "                    return {\"ok\": True, \"result\": translated_data}\n",
    "            return {\"ok\": False, \"error\": \"翻译失败\"}\n",
    "        except:\n",
    "            return {\"ok\": False, \"error\": \"翻译出错\"}\n",
    "    \n",
    "    def fix_data_english(self, data):\n",
    "        basic_fields = [\"meeting_summary\", \"key_topics\", \"decisions\", \"next_steps\", \"unresolved_issues\"]\n",
    "        for field in basic_fields:\n",
    "            if field not in data:\n",
    "                data[field] = [] if field != \"meeting_summary\" else \"\"\n",
    "        \n",
    "        for i, topic in enumerate(data.get(\"key_topics\", [])):\n",
    "            topic[\"id\"] = f\"topic_{i+1}\"\n",
    "            if \"related_tasks\" not in topic:\n",
    "                topic[\"related_tasks\"] = []\n",
    "            for j, task in enumerate(topic[\"related_tasks\"]):\n",
    "                task[\"id\"] = f\"task_{i+1}_{j+1}\"\n",
    "                task.setdefault(\"status\", \"pending\")\n",
    "                task.setdefault(\"assignee\", \"unassigned\")\n",
    "                task.setdefault(\"deadline\", \"unclear\")\n",
    "                task.setdefault(\"priority\", \"medium\")\n",
    "        \n",
    "        for decision in data.get(\"decisions\", []):\n",
    "            decision.setdefault(\"decision_maker\", \"unclear\")\n",
    "            decision.setdefault(\"impact\", \"unclear\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def fix_data_chinese(self, data):\n",
    "        basic_fields = [\"meeting_summary\", \"key_topics\", \"decisions\", \"next_steps\", \"unresolved_issues\"]\n",
    "        for field in basic_fields:\n",
    "            if field not in data:\n",
    "                data[field] = [] if field != \"meeting_summary\" else \"\"\n",
    "        \n",
    "        for i, topic in enumerate(data.get(\"key_topics\", [])):\n",
    "            topic[\"id\"] = f\"议题_{i+1}\"\n",
    "            if \"related_tasks\" not in topic:\n",
    "                topic[\"related_tasks\"] = []\n",
    "            for j, task in enumerate(topic[\"related_tasks\"]):\n",
    "                task[\"id\"] = f\"任务_{i+1}_{j+1}\"\n",
    "                task.setdefault(\"status\", \"待处理\")\n",
    "                task.setdefault(\"assignee\", \"未明确\")\n",
    "                task.setdefault(\"deadline\", \"未明确\")\n",
    "                task.setdefault(\"priority\", \"medium\")\n",
    "        \n",
    "        for decision in data.get(\"decisions\", []):\n",
    "            decision.setdefault(\"decision_maker\", \"未明确\")\n",
    "            decision.setdefault(\"impact\", \"未明确\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def simple_way_english(self, meeting_text):\n",
    "        try:\n",
    "            topic_answer = self.call_ai(f\"What are the main topics?\\n{meeting_text[:800]}\")\n",
    "            decision_answer = self.call_ai(f\"What decisions were made?\\n{meeting_text[:800]}\")\n",
    "            \n",
    "            return {\n",
    "                \"ok\": True,\n",
    "                \"result\": {\n",
    "                    \"meeting_summary\": \"Meeting analysis completed\",\n",
    "                    \"key_topics\": [{\n",
    "                        \"id\": \"topic_1\",\n",
    "                        \"topic\": \"Main Topics\", \n",
    "                        \"description\": topic_answer or \"Unable to extract\",\n",
    "                        \"importance\": \"medium\",\n",
    "                        \"related_tasks\": []\n",
    "                    }],\n",
    "                    \"decisions\": [{\n",
    "                        \"decision\": decision_answer or \"No clear decisions\", \n",
    "                        \"decision_maker\": \"unclear\", \n",
    "                        \"impact\": \"unclear\"\n",
    "                    }],\n",
    "                    \"next_steps\": [],\n",
    "                    \"unresolved_issues\": []\n",
    "                }\n",
    "            }\n",
    "        except:\n",
    "            return {\"ok\": False, \"error\": \"Simple extraction failed\"}\n",
    "    \n",
    "    def simple_way_chinese(self, meeting_text):\n",
    "        try:\n",
    "            topic_answer = self.call_ai(f\"主要讨论了什么？\\n{meeting_text[:800]}\")\n",
    "            decision_answer = self.call_ai(f\"做了什么决定？\\n{meeting_text[:800]}\")\n",
    "            \n",
    "            return {\n",
    "                \"ok\": True,\n",
    "                \"result\": {\n",
    "                    \"meeting_summary\": \"会议分析完成\",\n",
    "                    \"key_topics\": [{\n",
    "                        \"id\": \"议题_1\",\n",
    "                        \"topic\": \"主要议题\", \n",
    "                        \"description\": topic_answer or \"无法提取\",\n",
    "                        \"importance\": \"medium\",\n",
    "                        \"related_tasks\": []\n",
    "                    }],\n",
    "                    \"decisions\": [{\n",
    "                        \"decision\": decision_answer or \"无明确决策\", \n",
    "                        \"decision_maker\": \"未明确\", \n",
    "                        \"impact\": \"未明确\"\n",
    "                    }],\n",
    "                    \"next_steps\": [],\n",
    "                    \"unresolved_issues\": []\n",
    "                }\n",
    "            }\n",
    "        except:\n",
    "            return {\"ok\": False, \"error\": \"简单提取失败\"}\n",
    "    \n",
    "    def print_result_english(self, result):\n",
    "        print(f\"📝 Meeting Summary: {result.get('meeting_summary', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\n🎯 Key Topics:\")\n",
    "        for topic in result.get('key_topics', []):\n",
    "            print(f\"  • {topic.get('topic', 'Unknown')} ({topic.get('importance', 'medium')} priority)\")\n",
    "            if topic.get('related_tasks'):\n",
    "                for task in topic['related_tasks']:\n",
    "                    print(f\"    - {task.get('task', 'Unknown task')} → {task.get('assignee', 'Unassigned')}\")\n",
    "        \n",
    "        print(\"\\n✅ Decisions:\")\n",
    "        for decision in result.get('decisions', []):\n",
    "            print(f\"  • {decision.get('decision', 'Unknown decision')}\")\n",
    "        \n",
    "        if result.get('next_steps'):\n",
    "            print(\"\\n⏭️ Next Steps:\")\n",
    "            for step in result['next_steps']:\n",
    "                print(f\"  • {step}\")\n",
    "        \n",
    "        if result.get('unresolved_issues'):\n",
    "            print(\"\\n❓ Unresolved Issues:\")\n",
    "            for issue in result['unresolved_issues']:\n",
    "                print(f\"  • {issue}\")\n",
    "    \n",
    "    def print_result_chinese(self, result):\n",
    "        print(f\"📝 会议总结：{result.get('meeting_summary', '无')}\")\n",
    "        \n",
    "        print(\"\\n🎯 关键议题：\")\n",
    "        for topic in result.get('key_topics', []):\n",
    "            print(f\"  • {topic.get('topic', '未知议题')} (重要性：{topic.get('importance', 'medium')})\")\n",
    "            if topic.get('related_tasks'):\n",
    "                for task in topic['related_tasks']:\n",
    "                    print(f\"    - {task.get('task', '未知任务')} → {task.get('assignee', '未分配')}\")\n",
    "        \n",
    "        print(\"\\n✅ 决策事项：\")\n",
    "        for decision in result.get('decisions', []):\n",
    "            print(f\"  • {decision.get('decision', '未知决策')}\")\n",
    "        \n",
    "        if result.get('next_steps'):\n",
    "            print(\"\\n⏭️ 下一步计划：\")\n",
    "            for step in result['next_steps']:\n",
    "                print(f\"  • {step}\")\n",
    "        \n",
    "        if result.get('unresolved_issues'):\n",
    "            print(\"\\n❓ 未解决问题：\")\n",
    "            for issue in result['unresolved_issues']:\n",
    "                print(f\"  • {issue}\")\n",
    "\n",
    "def main():\n",
    "    print(\"🚀 会议分析助手启动\")\n",
    "    \n",
    "    api_key = input(\"请输入千问API密钥: \").strip()\n",
    "    if not api_key:\n",
    "        print(\"❌ API密钥不能为空\")\n",
    "        return\n",
    "    \n",
    "    helper = MeetingHelper(api_key)\n",
    "    helper.start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffd65f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Meeting Analysis Tool (V2 - Markdown Output)\n",
      "==================================================\n",
      "\n",
      "--- Analyzing English Meeting Text (expecting multiple chunks) ---\n",
      "\n",
      "Analysis Result (Markdown String - English):\n",
      "### Topic: Efficient AI Model Deployment Using Intel Platforms  \n",
      "\n",
      "The episode explores the collaboration between Hugging Face and Intel to optimize AI model deployment on Intel CPUs and AI accelerators. Intel's fourth-generation Xeon Scalable processors with Advanced Matrix Extensions (AMX) accelerate matrix multiplications and support data types like BFloat16 and Integer 8. The third-generation Gaudi AI accelerator (Gaudi 3) features 128 GB of RAM and enhanced performance in FP8 and BFloat16 formats. Optimum Intel and Optimum Habana libraries enable seamless integration of these technologies into existing workflows. A live demo showcased Text Generation Inference (TGI) on Gaudi 3 for deploying large language models like Llama 3.17B efficiently. The partnership focuses on providing scalable and efficient solutions for enterprise applications, such as retrieval-augmented generation (RAG), with CPUs offering versatility for diverse machine learning tasks.\n",
      "\n",
      "---\n",
      "\n",
      "### Topic: Accelerating GPT-LLM Deployments on Intel Xeon CPUs Using Optimum Intel  \n",
      "\n",
      "The discussion centers on accelerating generative large language model (GPT-LLM) deployments on Intel Xeon CPUs using Optimum Intel. Key aspects include exporting models to OpenVINO Intermediate Representation (IR), applying weight-only quantization (8-bit and 4-bit), and performing inference with latent consistency models. Creating an OpenVINO-compatible model involves setting public or private repositories, reducing model size from FP32 to INT8 (four times smaller). Quantization methods like 8-bit and 4-bit were explored, with options for dataset calibration and controlling the ratio of weights quantized to 4-bit vs. 8-bit to mitigate accuracy loss. Optimum Benchmark, a tool for benchmarking performance across frameworks (PyTorch, OpenVINO, etc.), was introduced. The benchmarking process includes static shape compilation, weight-only quantization, and numactl support for multi-node CPUs. Results demonstrate significant speedups, especially in the decode stage of models like GPT-2, achieving up to 2.5x improvements in throughput and 7x speedup in per-token latency. The session underscores the importance of benchmarking to identify the optimal framework for specific use cases.\n",
      "\n",
      "---\n",
      "\n",
      "### Topic: Overview of Intel AI Accelerators and Model Optimization Techniques  \n",
      "\n",
      "The discussion delves into Intel AI accelerators (Gaudi) as alternatives to GPUs for AI workloads, focusing on the latest G3 generation. These accelerators are supported in libraries like Optimum Habana and production inference solutions such as TGI. Features include FP8 support, multimodal models, and deterministic or sampled outputs for inference requests. Optimum Intel accelerates AI workloads on CPUs and GPUs using frameworks like Intel Extensions for PyTorch (IPEx) and OpenVINO. Topics covered include exporting models for graph-mode execution, integrating with Hugging Face ecosystems, and optimizing inference through quantization. Quantization methods discussed include weight-only, dynamic, and static approaches, with considerations for memory efficiency, speed, and energy consumption. The impact of quantization on model accuracy varies based on techniques and model characteristics, with examples showing potential improvements in certain cases.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "# os and pathlib are not needed\n",
    "# from datetime import datetime # Not needed\n",
    "\n",
    "class MeetingAnalyzerV2:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\"\n",
    "        # Increased max_tokens as summaries can be long, especially combined ones\n",
    "        self.default_parameters = {\"temperature\": 0.3, \"max_tokens\": 3500} \n",
    "\n",
    "    def detect_language(self, text):\n",
    "        # Simple language detection based on character counts\n",
    "        chinese_chars = len(re.findall(r'[\\u4e00-\\u9fff]', text))\n",
    "        english_chars = len(re.findall(r'[a-zA-Z]', text))\n",
    "        \n",
    "        if chinese_chars > english_chars:\n",
    "            return \"中文\"\n",
    "        elif english_chars > chinese_chars and english_chars > 20:\n",
    "            return \"英文\"\n",
    "        elif chinese_chars > 0 :\n",
    "            return \"中文\"\n",
    "        else:\n",
    "            return \"英文\"\n",
    "\n",
    "    def call_ai(self, prompt, parameters=None):\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {self.api_key}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        \n",
    "        current_parameters = self.default_parameters.copy()\n",
    "        if parameters:\n",
    "            current_parameters.update(parameters)\n",
    "            \n",
    "        data = {\n",
    "            \"model\": \"qwen-turbo\", # or qwen-long for very long contexts if needed/available\n",
    "            \"input\": {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
    "            \"parameters\": current_parameters\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # print(f\"🔄 Calling AI with prompt (first 100 chars): {prompt[:100]}...\")\n",
    "            resp = requests.post(self.api_url, headers=headers, json=data, timeout=120) # Increased timeout\n",
    "            # print(f\"📡 API Status: {resp.status_code}\")\n",
    "            \n",
    "            if resp.status_code == 200:\n",
    "                result = resp.json()\n",
    "                if 'output' in result:\n",
    "                    if 'choices' in result['output'] and result['output']['choices']:\n",
    "                        return result['output']['choices'][0]['message']['content']\n",
    "                    elif 'text' in result['output']:\n",
    "                        return result['output']['text']\n",
    "                print(f\"⚠️ Unexpected API response structure: {result}\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"❌ API Error: {resp.status_code} - {resp.text}\")\n",
    "                return f\"API_ERROR: {resp.status_code} - {resp.text}\" # Return error string\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ Request Exception: {str(e)}\")\n",
    "            return f\"REQUEST_EXCEPTION: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            print(f\"❌ AI Call Exception: {str(e)}\")\n",
    "            return f\"AI_CALL_EXCEPTION: {str(e)}\"\n",
    "        return None\n",
    "\n",
    "\n",
    "    def _split_text_into_chunks(self, text: str, max_words: int = 3000) -> list[str]:\n",
    "        \"\"\"Splits text into chunks of approximately max_words.\"\"\"\n",
    "        words = text.split() # Simple split by whitespace\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), max_words):\n",
    "            chunk_words = words[i:i + max_words]\n",
    "            chunks.append(\" \".join(chunk_words))\n",
    "        # print(f\"Split text into {len(chunks)} chunks.\")\n",
    "        return chunks\n",
    "\n",
    "    def _summarize_chunk_in_language(self, chunk_text: str, is_chinese_prompt: bool) -> str:\n",
    "        \"\"\"Summarizes a single chunk of text using Markdown format.\"\"\"\n",
    "        if is_chinese_prompt:\n",
    "            prompt = f\"\"\"\n",
    "请你扮演一个专业的会议记录员。\n",
    "针对以下文本片段，请提取核心议题和关键内容。\n",
    "严格按照以下Markdown格式输出，不要添加任何额外的解释、引言或结尾：\n",
    "\n",
    "**主题：** [此处填写识别出的主题]\n",
    "**具体内容：** [此处填写该主题下的详细内容摘要]\n",
    "\n",
    "文本片段：\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "请确保输出清晰、简洁，并严格遵守上述格式。\n",
    "\"\"\"\n",
    "        else: # English prompt\n",
    "            prompt = f\"\"\"\n",
    "You are a professional meeting summarizer.\n",
    "For the following text segment, extract the core topic and key content.\n",
    "Strictly use the following Markdown format for your output. Do not add any extra explanations, introductions, or conclusions:\n",
    "\n",
    "**Topic:** [Insert identified topic here]\n",
    "**Content:** [Insert detailed content summary for this topic here]\n",
    "\n",
    "Text Segment:\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "Ensure your output is clear, concise, and strictly adheres to the format above.\n",
    "\"\"\"\n",
    "        # print(f\"Summarizing chunk (lang: {'CH' if is_chinese_prompt else 'EN'})...\")\n",
    "        summary = self.call_ai(prompt)\n",
    "        if summary and not (summary.startswith(\"API_ERROR:\") or summary.startswith(\"REQUEST_EXCEPTION:\") or summary.startswith(\"AI_CALL_EXCEPTION:\")):\n",
    "            return summary.strip()\n",
    "        else:\n",
    "            # print(f\"Failed to summarize chunk or AI returned error: {summary}\")\n",
    "            error_msg_key = \"主题：分块摘要失败\" if is_chinese_prompt else \"**Topic:** Chunk Summary Failed\"\n",
    "            error_msg_val = f\"未能处理此文本块。错误：{summary if summary else '未知错误'}\" if is_chinese_prompt else f\"Could not process this text block. Error: {summary if summary else 'Unknown error'}\"\n",
    "            return f\"{error_msg_key}\\n**具体内容：** {error_msg_val}\"\n",
    "\n",
    "\n",
    "    def _consolidate_summaries_in_language(self, combined_summaries: str, is_chinese_prompt: bool) -> str:\n",
    "        \"\"\"Consolidates and deduplicates combined chunk summaries.\"\"\"\n",
    "        if is_chinese_prompt:\n",
    "            prompt = f\"\"\"\n",
    "你是一位专业的编辑，擅长整合和提炼信息。\n",
    "以下是多个文本片段的初步概括，每个概括都包含“主题”和“具体内容”。\n",
    "你的任务是：\n",
    "1. 仔细阅读所有概括。\n",
    "2. 识别并合并重复或高度相似的主题。\n",
    "3. 对每个独特或合并后的主题，生成一个最终的、精炼的总结。\n",
    "4. 最终输出必须严格遵循以下Markdown格式，每个主题占一组：\n",
    "\n",
    "### 主题句1\n",
    "具体内容1\n",
    "\n",
    "### 主题句2\n",
    "具体内容2\n",
    "...\n",
    "\n",
    "请确保最终输出流畅自然，逻辑清晰，并且只包含符合上述格式的Markdown内容。不要添加任何其他前缀、后缀、引言或解释性文字。\n",
    "\n",
    "待整合的概括内容如下：\n",
    "---\n",
    "{combined_summaries}\n",
    "---\n",
    "请开始整合和提炼。\n",
    "\"\"\"\n",
    "        else: # English prompt\n",
    "            prompt = f\"\"\"\n",
    "You are an expert editor skilled in synthesizing and refining information.\n",
    "Below are preliminary summaries from multiple text segments, each containing a \"Topic\" and \"Content\".\n",
    "Your tasks are to:\n",
    "1. Carefully read all the summaries.\n",
    "2. Identify and merge duplicate or highly similar topics.\n",
    "3. For each unique or merged topic, generate a final, concise summary.\n",
    "4. The final output must strictly follow this Markdown format, with each topic in its own group:\n",
    "\n",
    "### Topic Sentence 1\n",
    "Detailed content for topic 1\n",
    "\n",
    "### Topic Sentence 2\n",
    "Detailed content for topic 2\n",
    "...\n",
    "\n",
    "Ensure the final output is fluent, logical, and contains ONLY Markdown content adhering to the format above. Do not add any prefixes, suffixes, introductions, or explanatory text.\n",
    "\n",
    "Summaries to consolidate:\n",
    "---\n",
    "{combined_summaries}\n",
    "---\n",
    "Please begin consolidation and refinement.\n",
    "\"\"\"\n",
    "        # print(f\"Consolidating summaries (lang: {'CH' if is_chinese_prompt else 'EN'})...\")\n",
    "        # Use higher max_tokens for consolidation\n",
    "        final_result = self.call_ai(prompt, parameters={\"max_tokens\": 4000}) # qwen-turbo max is 8k tokens for context\n",
    "        if final_result and not (final_result.startswith(\"API_ERROR:\") or final_result.startswith(\"REQUEST_EXCEPTION:\") or final_result.startswith(\"AI_CALL_EXCEPTION:\")):\n",
    "            return final_result.strip()\n",
    "        else:\n",
    "            # print(f\"Failed to consolidate summaries or AI returned error: {final_result}\")\n",
    "            return f\"### 整合失败\\n无法完成最终摘要整合。原始合并摘要：\\n{combined_summaries}\\n错误：{final_result if final_result else '未知错误'}\" \\\n",
    "                if is_chinese_prompt \\\n",
    "                else f\"### Consolidation Failed\\nCould not complete final summary consolidation. Original combined summaries:\\n{combined_summaries}\\nError: {final_result if final_result else 'Unknown error'}\"\n",
    "\n",
    "\n",
    "    def analyze(self, meeting_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Analyzes meeting text by splitting, summarizing chunks, and consolidating.\n",
    "        Returns a Markdown string.\n",
    "        \"\"\"\n",
    "        if not meeting_text or not meeting_text.strip():\n",
    "            return \"### 错误\\n输入的会议内容为空。\"\n",
    "\n",
    "        language = self.detect_language(meeting_text)\n",
    "        is_chinese = (language == \"中文\")\n",
    "        # print(f\"🌐 Detected language: {language}\")\n",
    "\n",
    "        # 1. Split text into chunks\n",
    "        # Word count can be tricky for CJK languages. 3000 words might be ~6000-9000 characters.\n",
    "        # qwen-turbo has a context window of 8k tokens. Let's use a character limit for splitting\n",
    "        # to be safer, or stick to word count and hope the model handles tokenization well.\n",
    "        # For words, 3000 words is a good number.\n",
    "        chunks = self._split_text_into_chunks(meeting_text, max_words=2800) # Slightly less than 3000 for safety margin\n",
    "\n",
    "        if not chunks:\n",
    "            return \"### 错误\\n无法将文本分割成块。\" if is_chinese else \"### Error\\nCould not split text into chunks.\"\n",
    "\n",
    "        # 2. Summarize each chunk\n",
    "        chunk_summaries = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # print(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
    "            summary = self._summarize_chunk_in_language(chunk, is_chinese)\n",
    "            chunk_summaries.append(summary)\n",
    "        \n",
    "        combined_chunk_summaries = \"\\n\\n---\\n\\n\".join(chunk_summaries) # Separator for AI to distinguish\n",
    "\n",
    "        # 3. Consolidate and deduplicate summaries\n",
    "        final_markdown_summary = self._consolidate_summaries_in_language(combined_chunk_summaries, is_chinese)\n",
    "        \n",
    "        return final_markdown_summary\n",
    "\n",
    "\n",
    "def analyze_meeting_text_v2(meeting_content: str, api_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Main function to analyze meeting text (V2: Markdown multi-stage).\n",
    "    \n",
    "    Args:\n",
    "        meeting_content: The raw text of the meeting.\n",
    "        api_key: Your Dashscope API key.\n",
    "        \n",
    "    Returns:\n",
    "        A Markdown string of the analysis result, or an error message.\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        return \"### 错误\\nAPI密钥是必需的。\"\n",
    "    if not meeting_content or not meeting_content.strip():\n",
    "        return \"### 错误\\n会议内容不能为空。\"\n",
    "\n",
    "    analyzer = MeetingAnalyzerV2(api_key)\n",
    "    analysis_result_markdown = analyzer.analyze(meeting_content)\n",
    "    \n",
    "    return analysis_result_markdown\n",
    "\n",
    "# Example Usage (main part for testing):\n",
    "if __name__ == \"__main__\":\n",
    "    # IMPORTANT: Set your API key here or via an environment variable\n",
    "    try:\n",
    "        API_KEY = open('qwen.key').read().strip() \n",
    "        if not API_KEY or \"your_actual_api_key_here\" in API_KEY: # Basic check\n",
    "            raise FileNotFoundError \n",
    "    except FileNotFoundError:\n",
    "        API_KEY = \"sk-your_actual_api_key_here\" # Placeholder\n",
    "        print(\"🚨 API key file 'qwen.key' not found or empty. Please create it with your Dashscope API key.\")\n",
    "\n",
    "    if \"your_actual_api_key_here\" in API_KEY:\n",
    "        print(\"🚨 Please replace 'sk-your_actual_api_key_here' with your actual Dashscope API key to run the example.\")\n",
    "    else:\n",
    "        print(\"🚀 Meeting Analysis Tool (V2 - Markdown Output)\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Test with a sample English text file\n",
    "        try:\n",
    "            # Use a longer text file for meaningful chunking\n",
    "            # Create a dummy long_english_text.txt if you don't have one\n",
    "            # For example, repeat the content of 'huggingcast-s2e6 text-en.txt' multiple times\n",
    "            with open('huggingcast-s2e6 text-en.txt', 'r', encoding='utf-8') as f_en:\n",
    "                english_meeting_text_short = f_en.read()\n",
    "\n",
    "            print(\"\\n--- Analyzing English Meeting Text (expecting multiple chunks) ---\")\n",
    "            result_markdown_en = analyze_meeting_text_v2(english_meeting_text_short, API_KEY)\n",
    "            print(\"\\nAnalysis Result (Markdown String - English):\")\n",
    "            print(result_markdown_en)\n",
    "\n",
    "            # You can add a Chinese text example similarly if you have one\n",
    "            # chinese_meeting_text = \"\"\"\n",
    "            # [一段较长的中文会议记录，最好超过3000个“词”的概念，可能需要几千到一万个汉字]\n",
    "            # 第一次项目启动会讨论了关于新产品“智能助手”的开发计划。\n",
    "            # 张三：我认为我们首先需要明确产品的核心功能。是主打语音交互还是文本辅助？\n",
    "            # 李四：我同意张三的看法。市场调研显示，用户对高效语音交互的需求很高。建议初期集中资源攻克语音识别和自然语言理解的准确性。\n",
    "            # 王五：关于时间节点，我们能否在三个月内推出第一个beta版本？\n",
    "            # 赵六：三个月比较紧张，考虑到技术难点和团队磨合，我建议四个月，确保质量。\n",
    "            # ... (更多内容，确保文本足够长) ...\n",
    "            # 第二部分讨论了市场推广策略。\n",
    "            # 陈七：早期用户获取方面，我们考虑和一些科技KOL合作。\n",
    "            # 周八：社交媒体宣传也很重要，特别是针对年轻用户群体。\n",
    "            # ... (更多内容) ...\n",
    "            # 最终决定，产品核心功能侧重语音交互，开发周期定为四个月，市场推广初期以KOL合作为主。\n",
    "            # 下一步行动：李四负责整理详细的技术需求文档，下周一前完成。王五和赵六共同制定详细的项目排期。陈七调研合适的KOL资源。\n",
    "            # \"\"\"\n",
    "            # print(\"\\n--- Analyzing Chinese Meeting Text ---\")\n",
    "            # result_markdown_ch = analyze_meeting_text_v2(chinese_meeting_text * 5, API_KEY) # Multiply to make it long\n",
    "            # print(\"\\nAnalysis Result (Markdown String - Chinese):\")\n",
    "            # print(result_markdown_ch)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"\\n⚠️ English test file 'huggingcast-s2e6 text-en.txt' not found. Skipping English example.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ An error occurred during testing: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
